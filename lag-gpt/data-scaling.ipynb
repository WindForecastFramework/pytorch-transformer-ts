{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "experiment_name = \"data-scaling-uniform-0.5\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "s = [val_loss[\"step\"][0], val_loss[\"step\"][2], val_loss[\"step\"][9], val_loss[\"step\"][29], val_loss[\"step\"][99]]\n",
    "L = [val_loss[\"mean\"][0], val_loss[\"mean\"][2], val_loss[\"mean\"][9], val_loss[\"mean\"][29], val_loss[\"mean\"][99]]\n",
    "e = [val_loss[\"sd\"][0], val_loss[\"sd\"][2], val_loss[\"sd\"][9], val_loss[\"sd\"][29], val_loss[\"sd\"][99]]\n",
    "ax[0,0].errorbar(s, L, linestyle=\"--\", marker=\"o\", yerr = e)\n",
    "ax[0,0].set_xscale(\"log\")\n",
    "ax[0,0].set_ylabel(\"Test Loss\")\n",
    "ax[0,0].set_title(\"Transformer\")\n",
    "\n",
    "experiment_name = \"data-scaling-weighted-1.0\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "s = [val_loss[\"step\"][0], val_loss[\"step\"][2], val_loss[\"step\"][9], val_loss[\"step\"][29], val_loss[\"step\"][99]]\n",
    "L = [val_loss[\"mean\"][0], val_loss[\"mean\"][2], val_loss[\"mean\"][9], val_loss[\"mean\"][29], val_loss[\"mean\"][99]]\n",
    "e = [val_loss[\"sd\"][0], val_loss[\"sd\"][2], val_loss[\"sd\"][9], val_loss[\"sd\"][29], val_loss[\"sd\"][99]]\n",
    "ax[1,0].errorbar(s, L, linestyle=\"--\", marker=\"o\", yerr=e)\n",
    "ax[1,0].set_xscale(\"log\")\n",
    "ax[1,0].set_ylabel(\"Test Loss\")\n",
    "ax[1,0].set_xlabel(\"Steps\")\n",
    "\n",
    "experiment_name = \"data-scaling-uniform-0.5\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "s = [val_loss[\"step\"][0], val_loss[\"step\"][2], val_loss[\"step\"][9], val_loss[\"step\"][29], val_loss[\"step\"][99]]\n",
    "L = [val_loss[\"mean\"][0], val_loss[\"mean\"][2], val_loss[\"mean\"][9], val_loss[\"mean\"][29], val_loss[\"mean\"][99]]\n",
    "e = [val_loss[\"sd\"][0], val_loss[\"sd\"][2], val_loss[\"sd\"][9], val_loss[\"sd\"][29], val_loss[\"sd\"][99]]\n",
    "\n",
    "ax[0,1].errorbar(s, L, linestyle=\"--\", marker=\"o\", yerr=e)\n",
    "ax[0,1].set_xscale(\"log\")\n",
    "ax[0,1].set_title(\"GPT\")\n",
    "\n",
    "experiment_name = \"data-scaling-weighted-1.0\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "s = [val_loss[\"step\"][0], val_loss[\"step\"][2], val_loss[\"step\"][9], val_loss[\"step\"][29], val_loss[\"step\"][99]]\n",
    "L = [val_loss[\"mean\"][0], val_loss[\"mean\"][2], val_loss[\"mean\"][9], val_loss[\"mean\"][29], val_loss[\"mean\"][99]]\n",
    "e = [val_loss[\"sd\"][0], val_loss[\"sd\"][2], val_loss[\"sd\"][9], val_loss[\"sd\"][29], val_loss[\"sd\"][99]]\n",
    "\n",
    "ax[1,1].errorbar(s,L, linestyle=\"--\", marker=\"o\", yerr=e)\n",
    "ax[1,1].set_xscale(\"log\")\n",
    "ax[1,1].set_xlabel(\"Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"data-scaling-losses.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, c, a):\n",
    "    return  c*np.power(x, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "experiment_name = \"data-scaling-uniform-0.5\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "c, _ = scipy.optimize.curve_fit(objective, val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "x_line = np.arange(min(val_loss[\"step\"][:150]), max(val_loss[\"step\"][:150]), 1)\n",
    "y_line = objective(np.array(x_line), c[0], c[1])\n",
    "\n",
    "ax.plot(val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "ax.plot(x_line, y_line)\n",
    "ax.set_ylabel(\"Test Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "experiment_name = \"data-scaling-weighted-1.0\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-transformer/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "c, _ = scipy.optimize.curve_fit(objective, val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "x_line = np.arange(min(val_loss[\"step\"][:150]), max(val_loss[\"step\"][:150]), 1)\n",
    "y_line = objective(np.array(x_line), c[0], c[1])\n",
    "\n",
    "ax.plot(val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "ax.plot(x_line, y_line)\n",
    "ax.set_ylabel(\"Test Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.power(c[0], 1/-c[1])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(d, -c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "experiment_name = \"data-scaling-uniform-0.5\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "c, _ = scipy.optimize.curve_fit(objective, val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "x_line = np.arange(min(val_loss[\"step\"][:150]), max(val_loss[\"step\"][:150]), 1)\n",
    "y_line = objective(np.array(x_line), c[0], c[1])\n",
    "\n",
    "ax.plot(val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "ax.plot(x_line, y_line)\n",
    "ax.set_ylabel(\"Test Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "experiment_name = \"data-scaling-weighted-1.0\"\n",
    "\n",
    "experiment_version = 1\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_1 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 2\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_2 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 3\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_3 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 4\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_4 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "experiment_version = 5\n",
    "loss_df = pd.read_csv(\"lag-gpt/data-scaling-logs/\"+experiment_name+\"/version_\"+str(experiment_version)+\"/metrics.csv\")\n",
    "val_loss_5 = loss_df.dropna(subset=[\"val_loss\"])\n",
    "\n",
    "val_loss = pd.DataFrame({\"step\": val_loss_1[\"step\"], \"val_loss_1\": val_loss_1[\"val_loss\"], \"val_loss_2\": val_loss_2[\"val_loss\"], \"val_loss_3\": val_loss_3[\"val_loss\"], \"val_loss_4\": val_loss_4[\"val_loss\"], \"val_loss_5\": val_loss_5[\"val_loss\"] })\n",
    "val_loss['mean'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\" ]].mean(axis=1)\n",
    "val_loss['sd'] = val_loss[['val_loss_1', 'val_loss_2', \"val_loss_3\", \"val_loss_4\", \"val_loss_5\"]].std(axis=1)\n",
    "\n",
    "\n",
    "c, _ = scipy.optimize.curve_fit(objective, val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "x_line = np.arange(min(val_loss[\"step\"][:150]), max(val_loss[\"step\"][:150]), 1)\n",
    "y_line = objective(np.array(x_line), c[0], c[1])\n",
    "\n",
    "ax.plot(val_loss[\"step\"][:150], val_loss[\"mean\"][:150])\n",
    "ax.plot(x_line, y_line)\n",
    "ax.set_ylabel(\"Test Loss\")\n",
    "ax.set_xlabel(\"Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.power(c[0], 1/-c[1])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
