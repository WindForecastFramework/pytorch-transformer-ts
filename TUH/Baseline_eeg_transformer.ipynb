{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from braindecode.datasets import TUH\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "\n",
    "mne.set_log_level('ERROR')  # avoid messages everytime a window is extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets.tuh import _TUHMock as TUH  # noqa F811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUH_PATH = 'edf/train/'\n",
    "tuh = TUH(\n",
    "    path=TUH_PATH,\n",
    "    recording_ids=None,\n",
    "    target_name=('gender'),  # use both age and gender as decoding target\n",
    "    preload=False,\n",
    "    add_physician_reports=False,\n",
    ")\n",
    "tuh.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = tuh[-1]\n",
    "print('x:', x)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_windows = create_fixed_length_windows(\n",
    "    tuh,\n",
    "    start_offset_samples=0,\n",
    "    stop_offset_samples=None,\n",
    "    window_size_samples=1000,\n",
    "    window_stride_samples=1000,\n",
    "    drop_last_window=False,\n",
    "    mapping={'M': 0, 'F': 1},  # map non-digit targets\n",
    ")\n",
    "# store the number of windows required for loading later on\n",
    "tuh_windows.set_description({\n",
    "    \"n_windows\": [len(d) for d in tuh_windows.datasets]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, ind = tuh_windows[-1]\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('ind:', ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    dataset=tuh_windows,\n",
    "    batch_size=4,\n",
    ")\n",
    "for batch_X, batch_y, batch_ind in dl:\n",
    "    pass\n",
    "print('batch_X:', batch_X)\n",
    "print('batch_y:', batch_y)\n",
    "print('batch_ind:', batch_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUH_PATH = 'edf/dev/'\n",
    "tuh1 = TUH(\n",
    "    path=TUH_PATH,\n",
    "    recording_ids=None,\n",
    "    target_name=('gender'),  # use both age and gender as decoding target\n",
    "    preload=False,\n",
    "    add_physician_reports=False,\n",
    ")\n",
    "tuh1.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_windows1 = create_fixed_length_windows(\n",
    "    tuh1,\n",
    "    start_offset_samples=0,\n",
    "    stop_offset_samples=None,\n",
    "    window_size_samples=1000,\n",
    "    window_stride_samples=1000,\n",
    "    drop_last_window=False,\n",
    "    mapping={'M': 0, 'F': 1},  # map non-digit targets\n",
    ")\n",
    "# store the number of windows required for loading later on\n",
    "tuh_windows1.set_description({\n",
    "    \"n_windows\": [len(d) for d in tuh_windows1.datasets]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_windows1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dll = DataLoader(\n",
    "    dataset=tuh_windows1,\n",
    "    batch_size=4,\n",
    ")\n",
    "for batch_X1, batch_y1, batch_ind1 in dll:\n",
    "    batch_X1\n",
    "print('batch_X:', batch_X.shape)\n",
    "print('batch_y:', batch_y)\n",
    "print('batch_ind:', batch_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (20, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _ = x.shape\n",
    "        # print('x',x.shape)\n",
    "        x = x[ :,None, :, :]\n",
    "        # print('x',x.shape)\n",
    "        x = self.shallownet(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # global average pooling\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4880, 256),#4x4880\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return x, out\n",
    "\n",
    "\n",
    "class Conformer(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=6, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "\n",
    "            PatchEmbedding(emb_size),\n",
    "            TransformerEncoder(depth, emb_size),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExP():\n",
    "    def __init__(self, nsub):\n",
    "        super(ExP, self).__init__()\n",
    "        self.batch_size = 72\n",
    "        self.n_epochs = 500\n",
    "        self.c_dim = 4\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.dimension = (190, 50)\n",
    "        self.nSub = nsub\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.root = '/Data/strict_TE/'\n",
    "\n",
    "        # self.log_write = open(\"./results/log_subject%d.txt\" % self.nSub, \"w\")\n",
    "\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "        self.criterion_l1 = torch.nn.L1Loss().cuda()\n",
    "        self.criterion_l2 = torch.nn.MSELoss().cuda()\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "        gpus = [0]\n",
    "        self.model = Conformer().cuda()\n",
    "        self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
    "        self.model = self.model.cuda()\n",
    "        # summary(self.model, (1, 22, 1000))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        self.dataloader = DataLoader(dataset=tuh_windows,batch_size=4,)#torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_dataloader = DataLoader(dataset=tuh_windows1,batch_size=4,)#torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        bestAcc = 0\n",
    "        averAcc = 0\n",
    "        num = 0\n",
    "        Y_true = 0\n",
    "        Y_pred = 0\n",
    "\n",
    "        # Train the cnn model\n",
    "        total_step = len(self.dataloader)\n",
    "        curr_lr = self.lr\n",
    "\n",
    "        for e in range(self.n_epochs):\n",
    "            pred = []\n",
    "            true = []\n",
    "            \n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            for img, label, i in self.dataloader:\n",
    "\n",
    "                img = Variable(img.cuda().type(self.Tensor))\n",
    "                label = Variable(label.cuda().type(self.LongTensor))\n",
    "\n",
    "                # data augmentation\n",
    "                # aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # img = torch.cat((img, aug_data))\n",
    "                # label = torch.cat((label, aug_label))\n",
    "\n",
    "\n",
    "                tok, outputs = self.model(img)\n",
    "\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print('epoch',e)\n",
    "            print('loss', loss)\n",
    "            # out_epoch = time.time()\n",
    "\n",
    "#             for test_data, test_label, i in self.test_dataloader:\n",
    "#             # test process\n",
    "#                 if (e + 1) % 1 == 0:\n",
    "#                     self.model.eval()\n",
    "#                     Tok, Cls = self.model(test_data)\n",
    "\n",
    "\n",
    "#                     loss_test = self.criterion_cls(Cls, test_label)\n",
    "#                     y_pred = torch.max(Cls, 1)[1]\n",
    "#                     pred.append(y_pred)\n",
    "#                     true.append(test_label)\n",
    "                    \n",
    "#             acc = float((pred == true).cpu().numpy().astype(int).sum()) / float(len(true))\n",
    "#             # train_pred = torch.max(outputs, 1)[1]\n",
    "#             # train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "\n",
    "#             print('Epoch:', e,\n",
    "#                   '  Train loss: %.6f' % loss.detach().cpu().numpy(),\n",
    "#                   '  Test loss: %.6f' % loss_test.detach().cpu().numpy(),\n",
    "#                   # '  Train accuracy %.6f' % train_acc,\n",
    "#                   '  Test accuracy is %.6f' % acc)\n",
    "\n",
    "#             # self.log_write.write(str(e) + \"    \" + str(acc) + \"\\n\")\n",
    "#             num = num + 1\n",
    "#             averAcc = averAcc + acc\n",
    "#             if acc > bestAcc:\n",
    "#                 bestAcc = acc\n",
    "#                 Y_true = true\n",
    "#                 Y_pred = pred\n",
    "\n",
    "\n",
    "        torch.save(self.model.module.state_dict(), 'model.pth')\n",
    "#         averAcc = averAcc / num\n",
    "#         print('The average accuracy is:', averAcc)\n",
    "#         print('The best accuracy is:', bestAcc)\n",
    "#         # self.log_write.write('The average accuracy is: ' + str(averAcc) + \"\\n\")\n",
    "#         # self.log_write.write('The best accuracy is: ' + str(bestAcc) + \"\\n\")\n",
    "\n",
    "#         return bestAcc, averAcc, Y_true, Y_pred\n",
    "#         # writer.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    best = 0\n",
    "    aver = 0\n",
    "    # result_write = open(\"preprocess/sub_result.txt\", \"w\")\n",
    "\n",
    "    for i in range(1):\n",
    "        starttime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "        seed_n = np.random.randint(2021)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "\n",
    "\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1)\n",
    "\n",
    "        # bestAcc, averAcc, Y_true, Y_pred = \n",
    "        exp.train()\n",
    "#         print('THE BEST ACCURACY IS ' + str(bestAcc))\n",
    "#         # result_write.write('Subject ' + str(i + 1) + ' : ' + 'Seed is: ' + str(seed_n) + \"\\n\")\n",
    "#         # result_write.write('Subject ' + str(i + 1) + ' : ' + 'The best accuracy is: ' + str(bestAcc) + \"\\n\")\n",
    "#         # result_write.write('Subject ' + str(i + 1) + ' : ' + 'The average accuracy is: ' + str(averAcc) + \"\\n\")\n",
    "\n",
    "#         endtime = datetime.datetime.now()\n",
    "#         print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "#         best = best + bestAcc\n",
    "#         aver = aver + averAcc\n",
    "#         if i == 0:\n",
    "#             yt = Y_true\n",
    "#             yp = Y_pred\n",
    "#         else:\n",
    "#             yt = torch.cat((yt, Y_true))\n",
    "#             yp = torch.cat((yp, Y_pred))\n",
    "\n",
    "\n",
    "#     best = best / 9\n",
    "#     aver = aver / 9\n",
    "#     print('average best accuracy', best)\n",
    "#     print('average accuracy', aver)\n",
    "    # result_write.write('**The average Best accuracy is: ' + str(best) + \"\\n\")\n",
    "    # result_write.write('The average Aver accuracy is: ' + str(aver) + \"\\n\")\n",
    "    # result_write.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "    main()\n",
    "    print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Conformer()\n",
    "#self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
    "model = model#.cuda()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "true =[]\n",
    "for test_data, test_label, batch_ind1 in dll:\n",
    "    Tok, Cls = model(test_data)\n",
    "    # loss_test = self.criterion_cls(Cls, test_label)\n",
    "    y_pred = torch.max(Cls, 1)[1]\n",
    "    pred.append(y_pred.numpy())\n",
    "    true.append(test_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = float((np.concatenate(pred) == np.concatenate(true)).sum()) / float(len(true))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.concatenate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
