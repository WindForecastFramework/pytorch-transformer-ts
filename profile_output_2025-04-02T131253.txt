Timer unit: 1e-09 s

Total time: 146.935 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py
Function: _run at line 939

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   939                                               @profile
   940                                               def _run(
   941                                                   self, model: "pl.LightningModule", ckpt_path: Optional[_PATH] = None
   942                                               ) -> Optional[Union[_EVALUATE_OUTPUT, _PREDICT_OUTPUT]]:
   943         1      20000.0  20000.0      0.0          if self.state.fn == TrainerFn.FITTING:
   944         2      72000.0  36000.0      0.0              min_epochs, max_epochs = _parse_loop_limits(
   945         1     179000.0 179000.0      0.0                  self.min_steps, self.max_steps, self.min_epochs, self.max_epochs, self
   946                                                       )
   947         1       1000.0   1000.0      0.0              self.fit_loop.min_epochs = min_epochs
   948         1          0.0      0.0      0.0              self.fit_loop.max_epochs = max_epochs
   949                                           
   950         1       1000.0   1000.0      0.0          if self.barebones:
   951                                                       # no progress bar in barebones can make it look like the Trainer hung
   952                                                       rank_zero_info(
   953                                                           "`Trainer(barebones=True)` started running. The progress bar is disabled so you might want to"
   954                                                           " manually print the progress in your model."
   955                                                       )
   956                                           
   957                                                   # clean hparams
   958         1      12000.0  12000.0      0.0          if hasattr(model, "hparams"):
   959         1     576000.0 576000.0      0.0              parsing.clean_namespace(model.hparams)
   960                                           
   961                                                   # attach model to the strategy
   962         1      70000.0  70000.0      0.0          self.strategy.connect(model)
   963                                           
   964         1     464000.0 464000.0      0.0          self._callback_connector._attach_model_callbacks()
   965         1      53000.0  53000.0      0.0          self._callback_connector._attach_model_logging_functions()
   966                                           
   967         1    1774000.0    2e+06      0.0          _verify_loop_configurations(self)
   968                                           
   969                                                   # ----------------------------
   970                                                   # SET UP THE TRAINER
   971                                                   # ----------------------------
   972         1       4000.0   4000.0      0.0          log.debug(f"{self.__class__.__name__}: setting up strategy environment")
   973         1     116000.0 116000.0      0.0          self.strategy.setup_environment()
   974         1    2525000.0    3e+06      0.0          self.__setup_profiler()
   975                                           
   976         1       3000.0   3000.0      0.0          log.debug(f"{self.__class__.__name__}: preparing data")
   977         1     149000.0 149000.0      0.0          self._data_connector.prepare_data()
   978                                           
   979         1    2722000.0    3e+06      0.0          call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment
   980         1       2000.0   2000.0      0.0          log.debug(f"{self.__class__.__name__}: configuring model")
   981         1      70000.0  70000.0      0.0          call._call_configure_model(self)
   982                                           
   983                                                   # check if we should delay restoring checkpoint till later
   984         1      24000.0  24000.0      0.0          if not self.strategy.restore_checkpoint_after_setup:
   985         1       3000.0   3000.0      0.0              log.debug(f"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}")
   986         1     145000.0 145000.0      0.0              self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)
   987                                           
   988                                                   # reset logger connector
   989         1     255000.0 255000.0      0.0          self._logger_connector.reset_results()
   990         1      27000.0  27000.0      0.0          self._logger_connector.reset_metrics()
   991                                           
   992                                                   # strategy will configure model and move it to the device
   993         1    7394000.0    7e+06      0.0          self.strategy.setup(self)
   994                                           
   995                                                   # hook
   996         1       3000.0   3000.0      0.0          if self.state.fn == TrainerFn.FITTING:
   997         1    7431000.0    7e+06      0.0              call._call_callback_hooks(self, "on_fit_start")
   998         1      41000.0  41000.0      0.0              call._call_lightning_module_hook(self, "on_fit_start")
   999                                           
  1000         1  227359000.0    2e+08      0.2          _log_hyperparams(self)
  1001                                           
  1002         1       4000.0   4000.0      0.0          if self.strategy.restore_checkpoint_after_setup:
  1003                                                       log.debug(f"{self.__class__.__name__}: restoring module and callbacks from checkpoint path: {ckpt_path}")
  1004                                                       self._checkpoint_connector._restore_modules_and_callbacks(ckpt_path)
  1005                                           
  1006                                                   # restore optimizers, etc.
  1007         1       4000.0   4000.0      0.0          log.debug(f"{self.__class__.__name__}: restoring training state")
  1008         1      18000.0  18000.0      0.0          self._checkpoint_connector.restore_training_state()
  1009                                           
  1010         1      43000.0  43000.0      0.0          self._checkpoint_connector.resume_end()
  1011                                           
  1012         1     310000.0 310000.0      0.0          self._signal_connector.register_signal_handlers()
  1013                                           
  1014                                                   # ----------------------------
  1015                                                   # RUN THE TRAINER
  1016                                                   # ----------------------------
  1017         1        1e+11    1e+11     99.8          results = self._run_stage()
  1018                                           
  1019                                                   # ----------------------------
  1020                                                   # POST-Training CLEAN UP
  1021                                                   # ----------------------------
  1022         1       6000.0   6000.0      0.0          log.debug(f"{self.__class__.__name__}: trainer tearing down")
  1023         1    2892000.0    3e+06      0.0          self._teardown()
  1024                                           
  1025         1       4000.0   4000.0      0.0          if self.state.fn == TrainerFn.FITTING:
  1026         1      45000.0  45000.0      0.0              call._call_callback_hooks(self, "on_fit_end")
  1027         1      21000.0  21000.0      0.0              call._call_lightning_module_hook(self, "on_fit_end")
  1028                                           
  1029         1       1000.0   1000.0      0.0          log.debug(f"{self.__class__.__name__}: calling teardown hooks")
  1030         1     167000.0 167000.0      0.0          call._call_teardown_hook(self)
  1031                                           
  1032         1       1000.0   1000.0      0.0          self.state.status = TrainerStatus.FINISHED
  1033         1          0.0      0.0      0.0          self.state.stage = None
  1034                                           
  1035         1       2000.0   2000.0      0.0          return results

Total time: 146.938 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py
Function: _fit_impl at line 567

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   567                                               @profile
   568                                               def _fit_impl(
   569                                                   self,
   570                                                   model: "pl.LightningModule",
   571                                                   train_dataloaders: Optional[Union[TRAIN_DATALOADERS, LightningDataModule]] = None,
   572                                                   val_dataloaders: Optional[EVAL_DATALOADERS] = None,
   573                                                   datamodule: Optional[LightningDataModule] = None,
   574                                                   ckpt_path: Optional[_PATH] = None,
   575                                               ) -> None:
   576         1      41000.0  41000.0      0.0          log.debug(f"{self.__class__.__name__}: trainer fit stage")
   577                                           
   578                                                   # if a datamodule comes in as the second arg, then fix it for the user
   579         1       5000.0   5000.0      0.0          if isinstance(train_dataloaders, LightningDataModule):
   580                                                       datamodule = train_dataloaders
   581                                                       train_dataloaders = None
   582                                                   # If you supply a datamodule you can't supply train_dataloader or val_dataloaders
   583         1       1000.0   1000.0      0.0          if (train_dataloaders is not None or val_dataloaders is not None) and datamodule is not None:
   584                                                       raise MisconfigurationException(
   585                                                           "You cannot pass `train_dataloader` or `val_dataloaders` to `trainer.fit(datamodule=...)`"
   586                                                       )
   587                                           
   588                                                   # links data to the trainer
   589         2     513000.0 256500.0      0.0          self._data_connector.attach_data(
   590         1          0.0      0.0      0.0              model, train_dataloaders=train_dataloaders, val_dataloaders=val_dataloaders, datamodule=datamodule
   591                                                   )
   592                                           
   593         1       1000.0   1000.0      0.0          assert self.state.fn is not None
   594         1      52000.0  52000.0      0.0          if _is_registry(ckpt_path) and module_available("litmodels"):
   595                                                       download_model_from_registry(ckpt_path, self)
   596         2    1706000.0 853000.0      0.0          ckpt_path = self._checkpoint_connector._select_ckpt_path(
   597         1       1000.0   1000.0      0.0              self.state.fn,
   598         1       1000.0   1000.0      0.0              ckpt_path,
   599         1          0.0      0.0      0.0              model_provided=True,
   600         1      61000.0  61000.0      0.0              model_connected=self.lightning_module is not None,
   601                                                   )
   602         1        1e+11    1e+11    100.0          self._run(model, ckpt_path=ckpt_path)
   603                                           
   604         1      19000.0  19000.0      0.0          assert self.state.stopped
   605         1       5000.0   5000.0      0.0          self.training = False
   606         1          0.0      0.0      0.0          return

Total time: 146.939 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py
Function: fit at line 508

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   508                                               @profile
   509                                               def fit(
   510                                                   self,
   511                                                   model: "pl.LightningModule",
   512                                                   train_dataloaders: Optional[Union[TRAIN_DATALOADERS, LightningDataModule]] = None,
   513                                                   val_dataloaders: Optional[EVAL_DATALOADERS] = None,
   514                                                   datamodule: Optional[LightningDataModule] = None,
   515                                                   ckpt_path: Optional[_PATH] = None,
   516                                               ) -> None:
   517                                                   r"""Runs the full optimization routine.
   518                                           
   519                                                   Args:
   520                                                       model: Model to fit.
   521                                           
   522                                                       train_dataloaders: An iterable or collection of iterables specifying training samples.
   523                                                           Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines
   524                                                           the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.
   525                                           
   526                                                       val_dataloaders: An iterable or collection of iterables specifying validation samples.
   527                                           
   528                                                       datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines
   529                                                           the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.
   530                                           
   531                                                       ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of two special
   532                                                           keywords ``"last"``, ``"hpc"`` and ``"registry"``.
   533                                                           Otherwise, if there is no checkpoint file at the path, an exception is raised.
   534                                           
   535                                                               - best: the best model checkpoint from the previous ``trainer.fit`` call will be loaded
   536                                                               - last: the last model checkpoint from the previous ``trainer.fit`` call will be loaded
   537                                                               - registry: the model will be downloaded from the Lightning Model Registry with following notations:
   538                                           
   539                                                                   - ``'registry'``: uses the latest/default version of default model set
   540                                                                     with ``Tainer(..., model_registry="my-model")``
   541                                                                   - ``'registry:model-name'``: uses the latest/default version of this model `model-name`
   542                                                                   - ``'registry:model-name:version:v2'``: uses the specific version 'v2' of the model `model-name`
   543                                                                   - ``'registry:version:v2'``: uses the default model set
   544                                                                     with ``Tainer(..., model_registry="my-model")`` and version 'v2'
   545                                           
   546                                           
   547                                                   Raises:
   548                                                       TypeError:
   549                                                           If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than
   550                                                           2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or
   551                                                           :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .
   552                                           
   553                                                   For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.
   554                                           
   555                                                   """
   556         1      98000.0  98000.0      0.0          model = _maybe_unwrap_optimized(model)
   557         1      23000.0  23000.0      0.0          self.strategy._lightning_module = model
   558         1      66000.0  66000.0      0.0          _verify_strategy_supports_compile(model, self.strategy)
   559         1       6000.0   6000.0      0.0          self.state.fn = TrainerFn.FITTING
   560         1       6000.0   6000.0      0.0          self.state.status = TrainerStatus.RUNNING
   561         1      61000.0  61000.0      0.0          self.training = True
   562         1       4000.0   4000.0      0.0          self.should_stop = False
   563         2        1e+11    7e+10    100.0          call._call_and_handle_interrupt(
   564         1       1000.0   1000.0      0.0              self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
   565                                                   )

Total time: 151.665 s
File: /Users/ahenry/Documents/toolboxes/gluonts/src/gluonts/torch/model/estimator.py
Function: train_model at line 149

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   149                                               @profile
   150                                               def train_model(
   151                                                   self,
   152                                                   training_data: Dataset,
   153                                                   validation_data: Optional[Dataset] = None,
   154                                                   from_predictor: Optional[PyTorchPredictor] = None,
   155                                                   shuffle_buffer_length: Optional[int] = None,
   156                                                   cache_data: bool = False,
   157                                                   ckpt_path: Optional[str] = None,
   158                                                   **kwargs,
   159                                               ) -> TrainOutput:
   160         1    1609000.0    2e+06      0.0          transformation = self.create_transformation()
   161                                                    
   162         2     912000.0 456000.0      0.0          with env._let(max_idle_transforms=max(len(training_data), 100)):
   163         2      65000.0  32500.0      0.0              transformed_training_data: Dataset = transformation.apply(
   164         1       3000.0   3000.0      0.0                  training_data, is_train=True
   165                                                       )
   166                                                       # x = next(iter(transformed_training_data))
   167                                                        
   168         1       3000.0   3000.0      0.0              if cache_data:
   169                                                           transformed_training_data = Cached(transformed_training_data)
   170                                           
   171         1   15999000.0    2e+07      0.0              training_network = self.create_lightning_module()
   172                                           
   173                                                       
   174         2     709000.0 354500.0      0.0              training_data_loader = self.create_training_data_loader(
   175         1       2000.0   2000.0      0.0                  transformed_training_data,
   176         1       2000.0   2000.0      0.0                  training_network,
   177         1       2000.0   2000.0      0.0                  shuffle_buffer_length=shuffle_buffer_length,
   178                                                       )
   179                                                       # x = next(iter(training_data_loader))
   180                                                       
   181         1       2000.0   2000.0      0.0          validation_data_loader = None
   182                                           
   183         1       2000.0   2000.0      0.0          if validation_data is not None:
   184                                                       with env._let(max_idle_transforms=max(len(validation_data), 100)):
   185                                                           transformed_validation_data: Dataset = transformation.apply(
   186                                                               validation_data, is_train=True
   187                                                           )
   188                                                           if cache_data:
   189                                                               transformed_validation_data = Cached(
   190                                                                   transformed_validation_data
   191                                                               )
   192                                           
   193                                                           
   194                                                           validation_data_loader = self.create_validation_data_loader(
   195                                                               transformed_validation_data,
   196                                                               training_network,
   197                                                           )
   198                                           
   199         1       2000.0   2000.0      0.0          if from_predictor is not None:
   200                                                       training_network.load_state_dict(
   201                                                           from_predictor.network.state_dict()
   202                                                       )
   203                                           
   204         1       7000.0   7000.0      0.0          monitor = "train_loss" if validation_data is None else "val_loss"
   205         2    1799000.0 899500.0      0.0          checkpoint = pl.callbacks.ModelCheckpoint(
   206         1       2000.0   2000.0      0.0              monitor=monitor, mode="min", verbose=True
   207                                                   )
   208                                           
   209         1       4000.0   4000.0      0.0          custom_callbacks = self.trainer_kwargs.pop("callbacks", [])
   210         2  526661000.0    3e+08      0.3          trainer = pl.Trainer(
   211         2       5000.0   2500.0      0.0              **{
   212                                                           # "accelerator": "auto",
   213         1       3000.0   3000.0      0.0                  "callbacks": [checkpoint] + custom_callbacks,
   214         1       2000.0   2000.0      0.0                  **self.trainer_kwargs,
   215                                                       }
   216                                                   )
   217                                                   
   218         2        2e+11    8e+10     99.5          trainer.fit(
   219         1      28000.0  28000.0      0.0              model=training_network,
   220         1       7000.0   7000.0      0.0              train_dataloaders=training_data_loader,
   221         1       7000.0   7000.0      0.0              val_dataloaders=validation_data_loader,
   222         1       7000.0   7000.0      0.0              ckpt_path=ckpt_path,
   223                                                   )
   224                                           
   225         1      14000.0  14000.0      0.0          if checkpoint.best_model_path != "":
   226         2     135000.0  67500.0      0.0              logger.info(
   227         1       3000.0   3000.0      0.0                  f"Loading best model from {checkpoint.best_model_path}"
   228                                                       )
   229         2  158616000.0    8e+07      0.1              best_model = training_network.__class__.load_from_checkpoint(
   230         1       3000.0   3000.0      0.0                  checkpoint.best_model_path
   231                                                       )
   232                                                   else:
   233                                                       best_model = training_network
   234                                           
   235         2      28000.0  14000.0      0.0          return TrainOutput(
   236         1       2000.0   2000.0      0.0              transformation=transformation,
   237         1       2000.0   2000.0      0.0              trained_net=best_model,
   238         1       2000.0   2000.0      0.0              trainer=trainer,
   239         1    1858000.0    2e+06      0.0              predictor=self.create_predictor(transformation, best_model, **kwargs), # CHANGE
   240                                                   )

146.94 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:939 - _run
146.94 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:567 - _fit_impl
146.94 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:508 - fit
151.66 seconds - /Users/ahenry/Documents/toolboxes/gluonts/src/gluonts/torch/model/estimator.py:149 - train_model
