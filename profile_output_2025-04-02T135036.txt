Timer unit: 1e-09 s

Total time: 0.000145 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: reset at line 374

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   374                                               @profile
   375                                               def reset(self) -> None:
   376                                                   """Resets the internal state of this loop."""
   377         1      18000.0  18000.0     12.4          assert self.trainer.model is not None
   378         1      13000.0  13000.0      9.0          torch.set_grad_enabled(True)
   379                                           
   380         1      87000.0  87000.0     60.0          self.update_restart_stage()
   381                                           
   382         1       9000.0   9000.0      6.2          if self.restarted_on_epoch_start:
   383                                                       self.epoch_progress.reset_on_restart()
   384                                           
   385         1       8000.0   8000.0      5.5          if self.resumed_on_epoch_end:
   386                                                       # when restarting from last without validation at end of epoch,
   387                                                       # self.restarting is False but it's still resuming
   388                                                       self.epoch_progress.increment_completed()
   389                                           
   390                                                   if (
   391         1       8000.0   8000.0      5.5              self.epoch_loop.restarted_on_train_batch_end
   392                                                       and self.restarted_mid_epoch
   393                                                       and self.epoch_loop.batch_progress.is_last_batch
   394                                                   ):
   395                                                       self.epoch_progress.increment_processed()
   396                                                       self.epoch_progress.increment_completed()
   397                                           
   398                                                   if (
   399         1       2000.0   2000.0      1.4              self.epoch_loop.restarted_on_train_batch_end
   400                                                       and self.epoch_loop.batch_progress.is_last_batch
   401                                                       and not self.restarted_mid_epoch
   402                                                       and not self.epoch_loop.val_loop.batch_progress.is_last_batch
   403                                                   ):
   404                                                       self.epoch_progress.increment_completed()

Total time: 0.000164 s
File: /Users/ahenry/Documents/toolboxes/gluonts/src/gluonts/dataset/loader.py
Function: as_stacked_batches at line 59

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    59                                           @profile
    60                                           def as_stacked_batches(
    61                                               dataset: Dataset,
    62                                               *,
    63                                               batch_size: int,
    64                                               output_type: Optional[Callable] = None,
    65                                               num_batches_per_epoch: Optional[int] = None,
    66                                               shuffle_buffer_length: Optional[int] = None,
    67                                               field_names: Optional[list] = None,
    68                                           ):
    69                                               """
    70                                               Prepare data in batches to be passed to a network.
    71                                           
    72                                               Input data is collected into batches of size ``batch_size`` and then
    73                                               columns are stacked on top of each other. In addition, the result is
    74                                               wrapped in ``output_type`` if provided.
    75                                           
    76                                               If ``num_batches_per_epoch`` is provided, only those number of batches are
    77                                               effectively returned. This is especially useful for training when
    78                                               providing a cyclic dataset.
    79                                           
    80                                               To pseudo shuffle data, ``shuffle_buffer_length`` can be set to collect
    81                                               inputs into a buffer first, from which we then randomly sample.
    82                                           
    83                                               Setting ``field_names`` will only consider those columns in the input data
    84                                               and discard all other values.
    85                                               """
    86                                           
    87         1       1000.0   1000.0      0.6      if shuffle_buffer_length:
    88                                                   dataset = PseudoShuffled(dataset, shuffle_buffer_length)
    89                                           
    90         1          0.0      0.0      0.0      transform: Transformation = Identity()
    91                                           
    92         1          0.0      0.0      0.0      if field_names is not None:
    93         1      83000.0  83000.0     50.6          transform += SelectFields(field_names)
    94                                           
    95         1      14000.0  14000.0      8.5      transform += Batch(batch_size=batch_size)
    96         1       8000.0   8000.0      4.9      transform += Stack()
    97                                           
    98         1          0.0      0.0      0.0      if output_type is not None:
    99         1      47000.0  47000.0     28.7          transform += Valmap(output_type)
   100                                           
   101                                               # Note: is_train needs to be provided but does not have an effect
   102         1       1000.0   1000.0      0.6      transformed_dataset = transform.apply(dataset, is_train=True)
   103         1      10000.0  10000.0      6.1      return IterableSlice(transformed_dataset, num_batches_per_epoch)

Total time: 0.000494 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: on_run_end at line 498

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   498                                               @profile
   499                                               def on_run_end(self) -> None:
   500                                                   """Calls the ``on_train_end`` hook."""
   501         1       3000.0   3000.0      0.6          log.debug(f"{self.__class__.__name__}: train run ended")
   502                                           
   503         1          0.0      0.0      0.0          trainer = self.trainer
   504         1     444000.0 444000.0     89.9          call._call_callback_hooks(trainer, "on_train_end")
   505         1      24000.0  24000.0      4.9          call._call_lightning_module_hook(trainer, "on_train_end")
   506         1      23000.0  23000.0      4.7          call._call_strategy_hook(trainer, "on_train_end")

Total time: 0.000656 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: on_advance_start at line 425

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   425                                               @profile
   426                                               def on_advance_start(self) -> None:
   427                                                   """Prepares the dataloader for training and calls the hook ``on_train_epoch_start``"""
   428         1       1000.0   1000.0      0.2          trainer = self.trainer
   429                                           
   430                                                   # might need to setup data again depending on `trainer.reload_dataloaders_every_n_epochs`
   431         1      38000.0  38000.0      5.8          self.setup_data()
   432                                           
   433                                                   # update the epoch value for all samplers
   434         1          0.0      0.0      0.0          assert self._combined_loader is not None
   435         2      17000.0   8500.0      2.6          for i, dl in enumerate(self._combined_loader.flattened):
   436         1      33000.0  33000.0      5.0              _set_sampler_epoch(dl, self.epoch_progress.current.processed)
   437                                           
   438         1      21000.0  21000.0      3.2          if not self.restarted_mid_epoch and not self.restarted_on_epoch_end:
   439         1       1000.0   1000.0      0.2              if not self.restarted_on_epoch_start:
   440         1      17000.0  17000.0      2.6                  self.epoch_progress.increment_ready()
   441                                           
   442         1     475000.0 475000.0     72.4              call._call_callback_hooks(trainer, "on_train_epoch_start")
   443         1      36000.0  36000.0      5.5              call._call_lightning_module_hook(trainer, "on_train_epoch_start")
   444                                           
   445         1      17000.0  17000.0      2.6              self.epoch_progress.increment_started()

Total time: 0.036633 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: on_advance_end at line 463

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   463                                               @profile
   464                                               def on_advance_end(self) -> None:
   465         1       1000.0   1000.0      0.0          trainer = self.trainer
   466                                                   # inform logger the batch loop has finished
   467         1      12000.0  12000.0      0.0          trainer._logger_connector.epoch_end_reached()
   468                                           
   469         1       2000.0   2000.0      0.0          self.epoch_progress.increment_processed()
   470                                           
   471                                                   # call train epoch end hooks
   472                                                   # we always call callback hooks first, but here we need to make an exception for the callbacks that
   473                                                   # monitor a metric, otherwise they wouldn't be able to monitor a key logged in
   474                                                   # `LightningModule.on_train_epoch_end`
   475         1    6442000.0    6e+06     17.6          call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=False)
   476         1      31000.0  31000.0      0.1          call._call_lightning_module_hook(trainer, "on_train_epoch_end")
   477         1   29324000.0    3e+07     80.0          call._call_callback_hooks(trainer, "on_train_epoch_end", monitoring_callbacks=True)
   478                                           
   479         1      78000.0  78000.0      0.2          trainer._logger_connector.on_epoch_end()
   480                                           
   481         1       4000.0   4000.0      0.0          if not self.restarting and self.epoch_loop._num_ready_batches_reached():
   482                                                       # since metric-based schedulers require access to metrics and those are not currently saved in the
   483                                                       # checkpoint, the plateau schedulers shouldn't be updated
   484         1       4000.0   4000.0      0.0              self.epoch_loop.update_lr_schedulers("epoch", update_plateau_schedulers=not self.restarting)
   485                                           
   486                                                   # we manually decrease here because loggers expect that the same step is used when logging epoch-end metrics
   487                                                   # even when the batch loop has finished
   488         1          0.0      0.0      0.0          self.epoch_loop._batches_that_stepped -= 1
   489                                                   # log epoch metrics
   490         1     730000.0 730000.0      2.0          trainer._logger_connector.update_train_epoch_metrics()
   491         1       1000.0   1000.0      0.0          self.epoch_loop._batches_that_stepped += 1
   492                                           
   493         1       2000.0   2000.0      0.0          self.epoch_progress.increment_completed()
   494                                           
   495         1       2000.0   2000.0      0.0          if trainer.received_sigterm:
   496                                                       raise SIGTERMException

Total time: 0.041793 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: on_run_start at line 406

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   406                                               @profile
   407                                               def on_run_start(self) -> None:
   408                                                   """Calls the ``on_train_start`` hook."""
   409                                                   # update the current_epoch in-case of checkpoint reload
   410         1      14000.0  14000.0      0.0          if not self._iteration_based_training():
   411         1       1000.0   1000.0      0.0              self.epoch_progress.current.completed = self.epoch_progress.current.processed
   412                                           
   413         1          0.0      0.0      0.0          trainer = self.trainer
   414                                           
   415                                                   # reload the evaluation dataloaders too for proper display in the progress bar
   416         1      71000.0  71000.0      0.2          if self.epoch_loop._should_check_val_epoch() and trainer.val_dataloaders is None:
   417                                                       trainer.validating = True
   418                                                       self.epoch_loop.val_loop.setup_data()
   419                                                       trainer.training = True
   420                                           
   421         1   41533000.0    4e+07     99.4          call._call_callback_hooks(trainer, "on_train_start")
   422         1     106000.0 106000.0      0.3          call._call_lightning_module_hook(trainer, "on_train_start")
   423         1      68000.0  68000.0      0.2          call._call_strategy_hook(trainer, "on_train_start")

Total time: 0.08686 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loggers/utilities.py
Function: _log_hyperparams at line 58

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    58                                           @profile
    59                                           def _log_hyperparams(trainer: "pl.Trainer") -> None:
    60         1       1000.0   1000.0      0.0      if not trainer.loggers:
    61                                                   return
    62                                           
    63         1       1000.0   1000.0      0.0      pl_module = trainer.lightning_module
    64         1       1000.0   1000.0      0.0      datamodule_log_hyperparams = trainer.datamodule._log_hyperparams if trainer.datamodule is not None else False
    65                                           
    66         1          0.0      0.0      0.0      hparams_initial = None
    67         1       1000.0   1000.0      0.0      if pl_module._log_hyperparams and datamodule_log_hyperparams:
    68                                                   datamodule_hparams = trainer.datamodule.hparams_initial
    69                                                   lightning_hparams = pl_module.hparams_initial
    70                                                   inconsistent_keys = []
    71                                                   for key in lightning_hparams.keys() & datamodule_hparams.keys():
    72                                                       if key == "_class_path":
    73                                                           # Skip LightningCLI's internal hparam
    74                                                           continue
    75                                                       lm_val, dm_val = lightning_hparams[key], datamodule_hparams[key]
    76                                                       if (
    77                                                           type(lm_val) != type(dm_val)  # noqa: E721
    78                                                           or (isinstance(lm_val, Tensor) and id(lm_val) != id(dm_val))
    79                                                           or lm_val != dm_val
    80                                                       ):
    81                                                           inconsistent_keys.append(key)
    82                                                   if inconsistent_keys:
    83                                                       raise RuntimeError(
    84                                                           f"Error while merging hparams: the keys {inconsistent_keys} are present "
    85                                                           "in both the LightningModule's and LightningDataModule's hparams "
    86                                                           "but have different values."
    87                                                       )
    88                                                   hparams_initial = {**lightning_hparams, **datamodule_hparams}
    89         1          0.0      0.0      0.0      elif pl_module._log_hyperparams:
    90         1     270000.0 270000.0      0.3          hparams_initial = pl_module.hparams_initial
    91                                               elif datamodule_log_hyperparams:
    92                                                   hparams_initial = trainer.datamodule.hparams_initial
    93                                           
    94                                               # Don't log LightningCLI's internal hparam
    95         1          0.0      0.0      0.0      if hparams_initial is not None:
    96         4       2000.0    500.0      0.0          hparams_initial = {k: v for k, v in hparams_initial.items() if k != "_class_path"}
    97                                           
    98         2       2000.0   1000.0      0.0      for logger in trainer.loggers:
    99         1          0.0      0.0      0.0          if hparams_initial is not None:
   100         1   86504000.0    9e+07     99.6              logger.log_hyperparams(hparams_initial)
   101         1      19000.0  19000.0      0.0          logger.log_graph(pl_module)
   102         1      59000.0  59000.0      0.1          logger.save()

Total time: 3.24882 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: advance at line 447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   447                                               @profile
   448                                               def advance(self) -> None:
   449                                                   """Runs one whole epoch."""
   450         1       3000.0   3000.0      0.0          log.debug(f"{type(self).__name__}: advancing loop")
   451                                           
   452         1       1000.0   1000.0      0.0          combined_loader = self._combined_loader
   453         1          0.0      0.0      0.0          assert combined_loader is not None
   454         1       1000.0   1000.0      0.0          if combined_loader._mode == "sequential":
   455                                                       raise ValueError(
   456                                                           f'`{type(self).__name__}` does not support the `CombinedLoader(mode="sequential")` mode.'
   457                                                           f" The available modes are: {[m for m in _SUPPORTED_MODES if m != 'sequential']}"
   458                                                       )
   459         2       6000.0   3000.0      0.0          with self.trainer.profiler.profile("run_training_epoch"):
   460         1          0.0      0.0      0.0              assert self._data_fetcher is not None
   461         1 3248807000.0    3e+09    100.0              self.epoch_loop.run(self._data_fetcher)

Total time: 12.9065 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: setup_data at line 226

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   226                                               @profile
   227                                               def setup_data(self) -> None:
   228         2      20000.0  10000.0      0.0          if self._combined_loader is not None and not self._should_reload_train_dl:
   229         1       4000.0   4000.0      0.0              return
   230                                           
   231         1          0.0      0.0      0.0          trainer = self.trainer
   232         1       1000.0   1000.0      0.0          pl_module = trainer.lightning_module
   233         1       9000.0   9000.0      0.0          if trainer.limit_train_batches == 0 or not is_overridden("training_step", pl_module):
   234                                                       return
   235                                           
   236         1       6000.0   6000.0      0.0          log.debug(f"{self.__class__.__name__}: resetting train dataloader")
   237                                           
   238         1          0.0      0.0      0.0          source = self._data_source
   239         1     376000.0 376000.0      0.0          train_dataloader = _request_dataloader(source)
   240         1       1000.0   1000.0      0.0          trainer.strategy.barrier("train_dataloader()")
   241                                           
   242         1       5000.0   5000.0      0.0          if not isinstance(train_dataloader, CombinedLoader):
   243         1      97000.0  97000.0      0.0              combined_loader = CombinedLoader(train_dataloader, "max_size_cycle")
   244                                                   else:
   245                                                       combined_loader = train_dataloader
   246                                           
   247         1          0.0      0.0      0.0          if trainer.overfit_batches > 0:
   248                                                       _resolve_overfit_batches(combined_loader, mode=RunningStage.TRAINING)
   249                                           
   250         1       1000.0   1000.0      0.0          trainer_fn = TrainerFn.FITTING
   251         1       1000.0   1000.0      0.0          stage = RunningStage.TRAINING
   252         1          0.0      0.0      0.0          dataloaders = []
   253         2      12000.0   6000.0      0.0          for dl in combined_loader.flattened:
   254         1      39000.0  39000.0      0.0              _check_dataloader_iterable(dl, source, trainer_fn)
   255         1     106000.0 106000.0      0.0              dl = _process_dataloader(trainer, trainer_fn, stage, dl)
   256         1       1000.0   1000.0      0.0              dataloaders.append(dl)
   257         1      59000.0  59000.0      0.0          combined_loader.flattened = dataloaders
   258         1          0.0      0.0      0.0          self._combined_loader = combined_loader
   259                                           
   260         1          0.0      0.0      0.0          allow_zero_length = pl_module.allow_zero_length_dataloader_with_multiple_devices
   261         1       1000.0   1000.0      0.0          if trainer.datamodule is not None:
   262                                                       allow_zero_length |= trainer.datamodule.allow_zero_length_dataloader_with_multiple_devices
   263                                           
   264         1          0.0      0.0      0.0          limits = []
   265         2       2000.0   1000.0      0.0          for dl in combined_loader.flattened:
   266                                                       # determine number of batches
   267         1      38000.0  38000.0      0.0              length = len(dl) if has_len_all_ranks(dl, trainer.strategy, allow_zero_length) else float("inf")
   268         1      31000.0  31000.0      0.0              num_batches = _parse_num_batches(stage, length, trainer.limit_train_batches)
   269         1          0.0      0.0      0.0              limits.append(num_batches)
   270                                           
   271         1      35000.0  35000.0      0.0          combined_loader.limits = limits
   272                                           
   273         1      18000.0  18000.0      0.0          self._load_combined_loader_states()
   274                                           
   275         1     114000.0 114000.0      0.0          self._data_fetcher = _select_data_fetcher(trainer, RunningStage.TRAINING)
   276         1       8000.0   8000.0      0.0          self._data_fetcher.setup(combined_loader)
   277         1        1e+10    1e+10    100.0          iter(self._data_fetcher)  # creates the iterator inside the fetcher
   278         1     105000.0 105000.0      0.0          max_batches = sized_len(combined_loader)
   279         1       5000.0   5000.0      0.0          self.max_batches = max_batches if max_batches is not None else float("inf")
   280         1      50000.0  50000.0      0.0          has_len_all_ranks_ = has_len_all_ranks(combined_loader, trainer.strategy, allow_zero_length)
   281                                           
   282         1       4000.0   4000.0      0.0          if self.max_batches == 0:
   283                                                       return
   284                                           
   285                                                   # store epoch of dataloader reset for reload_dataloaders_every_n_epochs
   286         1      23000.0  23000.0      0.0          self._last_train_dl_reload_epoch = trainer.current_epoch
   287                                           
   288         1       4000.0   4000.0      0.0          if isinstance(trainer.val_check_interval, int):
   289                                                       trainer.val_check_batch = trainer.val_check_interval
   290                                                       if trainer.val_check_batch > self.max_batches and trainer.check_val_every_n_epoch is not None:
   291                                                           raise ValueError(
   292                                                               f" `val_check_interval` ({trainer.val_check_interval}) must be less than or equal"
   293                                                               f" to the number of the training batches ({self.max_batches})."
   294                                                               " If you want to disable validation set `limit_val_batches` to 0.0 instead."
   295                                                               " If you want to validate based on the total training batches, set `check_val_every_n_epoch=None`."
   296                                                           )
   297                                                   else:
   298         1       3000.0   3000.0      0.0              if not has_len_all_ranks_:
   299         1       3000.0   3000.0      0.0                  if trainer.val_check_interval == 1.0:
   300         1       6000.0   6000.0      0.0                      trainer.val_check_batch = float("inf")
   301                                                           else:
   302                                                               raise MisconfigurationException(
   303                                                                   "When using an IterableDataset for `train_dataloader`,"
   304                                                                   " `Trainer(val_check_interval)` must be `1.0` or an int. An int k specifies"
   305                                                                   " checking validation every k training batches."
   306                                                               )
   307                                                       else:
   308                                                           trainer.val_check_batch = int(self.max_batches * trainer.val_check_interval)
   309                                                           trainer.val_check_batch = max(1, trainer.val_check_batch)
   310                                           
   311         1      12000.0  12000.0      0.0          if trainer.loggers and self.max_batches < trainer.log_every_n_steps and not trainer.fast_dev_run:
   312                                                       rank_zero_warn(
   313                                                           f"The number of training batches ({self.max_batches}) is smaller than the logging interval"
   314                                                           f" Trainer(log_every_n_steps={trainer.log_every_n_steps}). Set a lower value for log_every_n_steps if"
   315                                                           " you want to see logs for the training epoch.",
   316                                                           category=PossibleUserWarning,
   317                                                       )

Total time: 16.2357 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py
Function: run at line 207

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   207                                               @profile
   208                                               def run(self) -> None:
   209         1        1e+10    1e+10     79.5          self.setup_data()
   210         1     118000.0 118000.0      0.0          if self.skip:
   211                                                       return
   212         1     184000.0 184000.0      0.0          self.reset()
   213         1   41873000.0    4e+07      0.3          self.on_run_start()
   214         2     117000.0  58500.0      0.0          while not self.done:
   215         1       2000.0   2000.0      0.0              try:
   216         1     696000.0 696000.0      0.0                  self.on_advance_start()
   217         1 3248861000.0    3e+09     20.0                  self.advance()
   218         1   36687000.0    4e+07      0.2                  self.on_advance_end()
   219                                                       except StopIteration:
   220                                                           break
   221                                                       finally:
   222         1      19000.0  19000.0      0.0                  self.on_iteration_done()
   223         1       2000.0   2000.0      0.0          self._restarting = False
   224         1     518000.0 518000.0      0.0          self.on_run_end()

Total time: 16.2367 s
File: /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py
Function: _run_stage at line 1044

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1044                                               @profile
  1045                                               def _run_stage(self) -> Optional[Union[_PREDICT_OUTPUT, _EVALUATE_OUTPUT]]:
  1046                                                   # wait for all to join if on distributed
  1047         1       1000.0   1000.0      0.0          self.strategy.barrier("run-stage")
  1048         1     510000.0 510000.0      0.0          self.lightning_module.zero_grad()
  1049                                           
  1050         1      32000.0  32000.0      0.0          if self.evaluating:
  1051                                                       return self._evaluation_loop.run()
  1052         1      11000.0  11000.0      0.0          if self.predicting:
  1053                                                       return self.predict_loop.run()
  1054         1       6000.0   6000.0      0.0          if self.training:
  1055         2     278000.0 139000.0      0.0              with isolate_rng():
  1056         1      62000.0  62000.0      0.0                  self._run_sanity_check()
  1057         2      71000.0  35500.0      0.0              with torch.autograd.set_detect_anomaly(self._detect_anomaly):
  1058         1        2e+10    2e+10    100.0                  self.fit_loop.run()
  1059         1       2000.0   2000.0      0.0              return None
  1060                                                   raise RuntimeError(f"Unexpected state {self.state}")

Total time: 16.8186 s
File: /Users/ahenry/Documents/toolboxes/gluonts/src/gluonts/torch/model/estimator.py
Function: train_model at line 149

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   149                                               @profile
   150                                               def train_model(
   151                                                   self,
   152                                                   training_data: Dataset,
   153                                                   validation_data: Optional[Dataset] = None,
   154                                                   from_predictor: Optional[PyTorchPredictor] = None,
   155                                                   shuffle_buffer_length: Optional[int] = None,
   156                                                   cache_data: bool = False,
   157                                                   ckpt_path: Optional[str] = None,
   158                                                   **kwargs,
   159                                               ) -> TrainOutput:
   160         1     880000.0 880000.0      0.0          transformation = self.create_transformation()
   161                                                    
   162         2     354000.0 177000.0      0.0          with env._let(max_idle_transforms=max(len(training_data), 100)):
   163         2      22000.0  11000.0      0.0              transformed_training_data: Dataset = transformation.apply(
   164         1          0.0      0.0      0.0                  training_data, is_train=True
   165                                                       )
   166                                                        
   167         1          0.0      0.0      0.0              if cache_data:
   168                                                           transformed_training_data = Cached(transformed_training_data)
   169                                           
   170         1    9189000.0    9e+06      0.1              training_network = self.create_lightning_module()
   171                                           
   172                                                       
   173         2     576000.0 288000.0      0.0              training_data_loader = self.create_training_data_loader(
   174         1          0.0      0.0      0.0                  transformed_training_data,
   175         1          0.0      0.0      0.0                  training_network,
   176         1       1000.0   1000.0      0.0                  shuffle_buffer_length=shuffle_buffer_length,
   177                                                       )
   178                                                       # x = next(iter(training_data_loader))
   179                                                       
   180         1          0.0      0.0      0.0          validation_data_loader = None
   181                                           
   182         1          0.0      0.0      0.0          if validation_data is not None:
   183                                                       with env._let(max_idle_transforms=max(len(validation_data), 100)):
   184                                                           transformed_validation_data: Dataset = transformation.apply(
   185                                                               validation_data, is_train=True
   186                                                           )
   187                                                           if cache_data:
   188                                                               transformed_validation_data = Cached(
   189                                                                   transformed_validation_data
   190                                                               )
   191                                           
   192                                                           
   193                                                           validation_data_loader = self.create_validation_data_loader(
   194                                                               transformed_validation_data,
   195                                                               training_network,
   196                                                           )
   197                                           
   198         1          0.0      0.0      0.0          if from_predictor is not None:
   199                                                       training_network.load_state_dict(
   200                                                           from_predictor.network.state_dict()
   201                                                       )
   202                                           
   203         1       1000.0   1000.0      0.0          monitor = "train_loss" if validation_data is None else "val_loss"
   204         2    1553000.0 776500.0      0.0          checkpoint = pl.callbacks.ModelCheckpoint(
   205         1          0.0      0.0      0.0              monitor=monitor, mode="min", verbose=True
   206                                                   )
   207                                           
   208         1       1000.0   1000.0      0.0          custom_callbacks = self.trainer_kwargs.pop("callbacks", [])
   209         2  327027000.0    2e+08      1.9          trainer = pl.Trainer(
   210         2       1000.0    500.0      0.0              **{
   211                                                           # "accelerator": "auto",
   212         1          0.0      0.0      0.0                  "callbacks": [checkpoint] + custom_callbacks,
   213         1          0.0      0.0      0.0                  **self.trainer_kwargs,
   214                                                       }
   215                                                   )
   216                                                   
   217         2        2e+10    8e+09     97.1          trainer.fit(
   218         1          0.0      0.0      0.0              model=training_network,
   219         1          0.0      0.0      0.0              train_dataloaders=training_data_loader,
   220         1          0.0      0.0      0.0              val_dataloaders=validation_data_loader,
   221         1          0.0      0.0      0.0              ckpt_path=ckpt_path,
   222                                                   )
   223                                           
   224         1      13000.0  13000.0      0.0          if checkpoint.best_model_path != "":
   225         2      87000.0  43500.0      0.0              logger.info(
   226         1       2000.0   2000.0      0.0                  f"Loading best model from {checkpoint.best_model_path}"
   227                                                       )
   228         2  138371000.0    7e+07      0.8              best_model = training_network.__class__.load_from_checkpoint(
   229         1       2000.0   2000.0      0.0                  checkpoint.best_model_path
   230                                                       )
   231                                                   else:
   232                                                       best_model = training_network
   233                                           
   234         2      22000.0  11000.0      0.0          return TrainOutput(
   235         1       2000.0   2000.0      0.0              transformation=transformation,
   236         1       2000.0   2000.0      0.0              trained_net=best_model,
   237         1       2000.0   2000.0      0.0              trainer=trainer,
   238         1    2980000.0    3e+06      0.0              predictor=self.create_predictor(transformation, best_model, **kwargs), # CHANGE
   239                                                   )

  0.00 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:374 - reset
  0.00 seconds - /Users/ahenry/Documents/toolboxes/gluonts/src/gluonts/dataset/loader.py:59 - as_stacked_batches
  0.00 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:498 - on_run_end
  0.00 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:425 - on_advance_start
  0.04 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:463 - on_advance_end
  0.04 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:406 - on_run_start
  0.09 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loggers/utilities.py:58 - _log_hyperparams
  3.25 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:447 - advance
 12.91 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:226 - setup_data
 16.24 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:207 - run
 16.24 seconds - /Users/ahenry/miniconda3/envs/wind_forecasting_env/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1044 - _run_stage
 16.82 seconds - /Users/ahenry/Documents/toolboxes/gluonts/src/gluonts/torch/model/estimator.py:149 - train_model
