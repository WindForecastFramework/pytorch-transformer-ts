{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from itertools import islice\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "from datasets import load_dataset\n",
    "from datasets.iterable_dataset import RandomlyCyclingMultiSourcesExamplesIterable\n",
    "\n",
    "\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.dataset.common import ListDataset, Dataset, DatasetCollection, Cached\n",
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    SecondOfMinute,\n",
    "    MinuteOfHour,\n",
    "    HourOfDay,\n",
    "    DayOfWeek,\n",
    "    DayOfMonth,\n",
    "    WeekOfYear,\n",
    "    MonthOfYear,\n",
    "    DayOfYear,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddTimeFeatures,\n",
    "    Chain,\n",
    ")\n",
    "\n",
    "from estimator import TransformerEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = [\n",
    "    MinuteOfHour(),\n",
    "    HourOfDay(),\n",
    "    DayOfWeek(),\n",
    "    DayOfMonth(),\n",
    "    WeekOfYear(),\n",
    "    MonthOfYear(),\n",
    "    DayOfYear(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache\n",
    "def as_period(val, freq):\n",
    "    return pd.Period(val, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GluontsDataset(Dataset):\n",
    "    def __init__(self, dataset, freq, prediction_length=24) -> None:\n",
    "        super().__init__()\n",
    "        transform = Chain([\n",
    "             AddTimeFeatures(\n",
    "                    start_field=FieldName.START,\n",
    "                    target_field=FieldName.TARGET,\n",
    "                    output_field=FieldName.FEAT_TIME,\n",
    "                    time_features=time_features,\n",
    "                    pred_length=prediction_length,\n",
    "                ),\n",
    "                AddAgeFeature(\n",
    "                    target_field=FieldName.TARGET,\n",
    "                    output_field=FieldName.FEAT_AGE,\n",
    "                    pred_length=prediction_length,\n",
    "                    log_scale=True,\n",
    "                ),\n",
    "        ])\n",
    "\n",
    "        self.dataset = Cached(transform.apply(dataset))\n",
    "        self.freq = to_offset(freq)\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.dataset:\n",
    "            if len(data[FieldName.TARGET]) > self.prediction_length:\n",
    "                yield {\n",
    "                    FieldName.START: as_period(data[FieldName.START], self.freq),\n",
    "                    FieldName.TARGET: data[FieldName.TARGET],\n",
    "                    FieldName.FEAT_TIME: np.stack(data[FieldName.FEAT_TIME], 0),\n",
    "                    FieldName.FEAT_AGE: np.stack(data[FieldName.FEAT_AGE], 0),\n",
    "                    FieldName.ITEM_ID: data[FieldName.ITEM_ID],\n",
    "                }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length  = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = get_dataset(\"electricity\")\n",
    "train_ds_1 = GluontsDataset(dataset_1.train, dataset_1.metadata.freq, prediction_length)\n",
    "test_ds_1 = GluontsDataset(dataset_1.test, dataset_1.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = get_dataset(\"traffic\")\n",
    "train_ds_2 = GluontsDataset(dataset_2.train, dataset_2.metadata.freq, prediction_length)\n",
    "test_ds_2 = GluontsDataset(dataset_2.test, dataset_2.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = get_dataset(\"m4_hourly\")\n",
    "train_ds_3 = GluontsDataset(dataset_3.train, dataset_3.metadata.freq, prediction_length)\n",
    "test_ds_3 = GluontsDataset(dataset_3.test, dataset_3.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = get_dataset(\"m4_daily\")\n",
    "train_ds_4 = GluontsDataset(dataset_4.train, dataset_4.metadata.freq, prediction_length)\n",
    "test_ds_4 = GluontsDataset(dataset_4.test, dataset_4.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = get_dataset(\"m4_weekly\")\n",
    "train_ds_5 = GluontsDataset(dataset_5.train, dataset_5.metadata.freq, prediction_length)\n",
    "test_ds_5 = GluontsDataset(dataset_5.test, dataset_5.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_6 = get_dataset(\"m4_monthly\")\n",
    "train_ds_6 = GluontsDataset(dataset_6.train, dataset_6.metadata.freq, prediction_length)\n",
    "test_ds_6 = GluontsDataset(dataset_6.test, dataset_6.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_7 = get_dataset(\"m4_quarterly\")\n",
    "train_ds_7 = GluontsDataset(dataset_7.train, dataset_7.metadata.freq, prediction_length)\n",
    "test_ds_7 = GluontsDataset(dataset_7.test, dataset_7.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_8 = get_dataset(\"solar-energy\")\n",
    "train_ds_8 = GluontsDataset(dataset_8.train, dataset_8.metadata.freq, prediction_length)\n",
    "test_ds_8 = GluontsDataset(dataset_8.test, dataset_8.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_9 = get_dataset(\"nn5_daily_with_missing\")\n",
    "train_ds_9 = GluontsDataset(dataset_9.train, dataset_9.metadata.freq, prediction_length)\n",
    "test_ds_9 = GluontsDataset(dataset_9.test, dataset_9.metadata.freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds_list = [train_ds_1, train_ds_2, train_ds_3, train_ds_4, train_ds_5, train_ds_6, train_ds_7, train_ds_8, train_ds_9]\n",
    "\n",
    "train_ds_list = [ train_ds_7, train_ds_9]\n",
    "\n",
    "train_ds_size = np.array([len(ds) for ds in train_ds_list])\n",
    "raw_weights = 1/train_ds_size\n",
    "normalization_factor = 1/sum(raw_weights)\n",
    "probabilities = raw_weights * normalization_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RandomlyCyclingMultiSourcesExamplesIterable(train_ds_list,\n",
    "    generator=np.random.default_rng(),\n",
    "    probabilities=[7/8, 1/8],\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_ds = ListDataset(dataset[\"validation\"], freq=freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = DatasetCollection([test_ds_1, test_ds_2, test_ds_3, test_ds_4, test_ds_5, test_ds_6, test_ds_7, test_ds_8, test_ds_9], interleave=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TransformerEstimator(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=prediction_length*10,\n",
    "    time_features=time_features,\n",
    "    lags_seq=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 24, 30, 31, 60,],\n",
    "\n",
    "    nhead=2,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=16,\n",
    "    activation=\"gelu\",\n",
    "\n",
    "    scaling=True,\n",
    "\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=200,\n",
    "    trainer_kwargs=dict(max_epochs=100, accelerator='auto', gpus=1, precision=\"bf16\"),\n",
    "    )\n",
    "    \n",
    "predictor = estimator.train(\n",
    "    training_data=train_ds,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds_7, \n",
    "    predictor=predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(forecast_it)\n",
    "evaluator = Evaluator()\n",
    "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)),9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)),9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Scaling Experiments\n",
    "\n",
    "We keep the individual layers of the Transformer unchanged: i.e. \n",
    "* the model dimension: context window (effecting lag features);\n",
    "* the width of the feed-forward layer `dim_feedforward=16`; \n",
    "* the number of attention heads `nhead=2`;\n",
    "* the categorical feature embedding dimension;\n",
    "* and distribution head.\n",
    "\n",
    "We examine the change in the test-set metrics as the number of parameters increases with the following three depth scaling approaches:\n",
    "1. Encoder Scaling: vary the `num_encoder_layers` while `num_decoder_layers` is kept fixed;\n",
    "1. Decoder Scaling: vary the `num_decoder_layers` while the `num_encoder_layers` is kept fixed;\n",
    "1. Symmetric Scaling: vary both the `num_encoder_layers` and `num_decoder_layers` but kept equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 48, 56, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Encoder Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_metrics = []\n",
    "for layer in layers:\n",
    "    estimator = TransformerEstimator(\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=prediction_length*10,\n",
    "        time_features=time_features,\n",
    "\n",
    "        nhead=2,\n",
    "        num_encoder_layers=layer,\n",
    "        num_decoder_layers=4,\n",
    "        dim_feedforward=16,\n",
    "        activation=\"gelu\",\n",
    "\n",
    "\n",
    "        batch_size=256,\n",
    "        num_batches_per_epoch=100,\n",
    "        trainer_kwargs=dict(max_epochs=10, accelerator='auto', gpus=1, precision=\"bf16\"),\n",
    "    )\n",
    "    \n",
    "    predictor = estimator.train(\n",
    "        training_data=train_ds,\n",
    "        num_workers=8,\n",
    "        shuffle_buffer_length=1024\n",
    "    )\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds, \n",
    "        predictor=predictor\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    \n",
    "    if layer == layers[0]:\n",
    "        tss = list(ts_it)\n",
    "    \n",
    "    evaluator = Evaluator()\n",
    "    agg_metrics, _ = evaluator(iter(tss), iter(forecasts))\n",
    "    agg_metrics[\"trainable_parameters\"] = summarize(estimator.create_lightning_module()).trainable_parameters\n",
    "    enc_metrics.append(agg_metrics.copy())\n",
    "    \n",
    "with open(\"elec_enc_metrics.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(enc_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elec_enc_metrics.pkl\", \"rb\") as fp:\n",
    "    enc_metrics = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in enc_metrics],\n",
    "    [metrics[\"mean_wQuantileLoss\"] for metrics in enc_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"CRPS\")\n",
    "plt.title(\"Encoder Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in enc_metrics],\n",
    "    [metrics[\"MASE\"] for metrics in enc_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"Encoder Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in enc_metrics],\n",
    "    [metrics[\"NRMSE\"] for metrics in enc_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"NRMSE\")\n",
    "plt.title(\"Encoder Scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Decoder Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_metrics = []\n",
    "for layer in layers:\n",
    "    estimator = TransformerEstimator(\n",
    "        freq=freq,\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=prediction_length*7,\n",
    "\n",
    "        nhead=2,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=layer,\n",
    "        dim_feedforward=16,\n",
    "        activation=\"gelu\",\n",
    "\n",
    "        num_feat_static_cat=1,\n",
    "        cardinality=[320],\n",
    "        embedding_dimension=[5],\n",
    "\n",
    "        batch_size=128,\n",
    "        num_batches_per_epoch=100,\n",
    "        trainer_kwargs=dict(max_epochs=50, accelerator='auto', gpus=1),\n",
    "    )\n",
    "    \n",
    "    predictor = estimator.train(\n",
    "        training_data=train_ds,\n",
    "        validation_data=val_ds,\n",
    "        num_workers=8,\n",
    "        shuffle_buffer_length=1024\n",
    "    )\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds, \n",
    "        predictor=predictor\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    if layer == layers[0]:\n",
    "        tss = list(ts_it)\n",
    "    \n",
    "    evaluator = Evaluator()\n",
    "    agg_metrics, _ = evaluator(iter(tss), iter(forecasts))\n",
    "    agg_metrics[\"trainable_parameters\"] = summarize(estimator.create_lightning_module()).trainable_parameters\n",
    "    dec_metrics.append(agg_metrics.copy())\n",
    "    \n",
    "with open(\"elec_dec_metrics.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(dec_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elec_dec_metrics.pkl\", \"rb\") as fp:\n",
    "    dec_metrics = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in dec_metrics],\n",
    "    [metrics[\"mean_wQuantileLoss\"] for metrics in dec_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"CRPS\")\n",
    "plt.title(\"Decoder Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in dec_metrics],\n",
    "    [metrics[\"MASE\"] for metrics in dec_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"Decoder Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in dec_metrics],\n",
    "    [metrics[\"NRMSE\"] for metrics in dec_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"NRMSE\")\n",
    "plt.title(\"Decoder Scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Symmetric Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_metrics = []\n",
    "for layer in layers:\n",
    "    estimator = TransformerEstimator(\n",
    "        freq=freq,\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=prediction_length*7,\n",
    "\n",
    "        nhead=2,\n",
    "        num_encoder_layers=layer,\n",
    "        num_decoder_layers=layer,\n",
    "        dim_feedforward=16,\n",
    "        activation=\"gelu\",\n",
    "\n",
    "        num_feat_static_cat=1,\n",
    "        cardinality=[320],\n",
    "        embedding_dimension=[5],\n",
    "\n",
    "        batch_size=128,\n",
    "        num_batches_per_epoch=100,\n",
    "        trainer_kwargs=dict(max_epochs=50, accelerator='auto', gpus=1),\n",
    "    )\n",
    "    \n",
    "    predictor = estimator.train(\n",
    "        training_data=train_ds,\n",
    "        validation_data=val_ds,\n",
    "        num_workers=8,\n",
    "        shuffle_buffer_length=1024\n",
    "    )\n",
    "    \n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds, \n",
    "        predictor=predictor\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    if layer == layers[0]:\n",
    "        tss = list(ts_it)\n",
    "    \n",
    "    evaluator = Evaluator()\n",
    "    agg_metrics, _ = evaluator(iter(tss), iter(forecasts))\n",
    "    agg_metrics[\"trainable_parameters\"] = summarize(estimator.create_lightning_module()).trainable_parameters\n",
    "    sym_metrics.append(agg_metrics.copy())\n",
    "\n",
    "with open(\"elec_sym_metrics.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(sym_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"elec_sym_metrics.pkl\", \"rb\") as fp:\n",
    "    sym_metrics = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in sym_metrics],\n",
    "    [metrics[\"mean_wQuantileLoss\"] for metrics in sym_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"CRPS\")\n",
    "plt.title(\"Symmetric Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in sym_metrics],\n",
    "    [metrics[\"MASE\"] for metrics in sym_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"MASE\")\n",
    "plt.title(\"Symmetric Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [metrics[\"trainable_parameters\"] for metrics in sym_metrics],\n",
    "    [metrics[\"NRMSE\"] for metrics in sym_metrics],   \n",
    ")\n",
    "plt.xlabel(\"trainable parameters\")\n",
    "plt.ylabel(\"NRMSE\")\n",
    "plt.title(\"Symmetric Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "40e4fedf32e21c0b32c442446148826532e66f2735982b93501b453fc06f0092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
