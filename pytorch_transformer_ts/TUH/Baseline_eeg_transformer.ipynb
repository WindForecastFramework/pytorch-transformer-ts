{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfe9f9d-0ac0-4ad3-b412-0a3059bfa2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ccs/proj/csc499/hstellar/rapids/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3427ef8e-a3b1-48fd-97ac-a7c9a542393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from braindecode.datasets import TUH\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "\n",
    "mne.set_log_level('ERROR')  # avoid messages everytime a window is extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea60267-2307-4705-afb0-96a9322a97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b730f64-2975-49d9-804b-59cd14d1bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154ade09-5a17-4810-82eb-6c668d8dcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.backends import cudnn\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f48aa8-de04-4254-8b1f-16819914d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets.tuh import _TUHMock as TUH  # noqa F811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59abb023-3e64-4744-abff-727268364d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>version</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>segment</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/02_tcp_le/000/00000058/s001...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/01_tcp_ar/099/00009932/s004...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>9932</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/03_tcp_ar_a/123/00012331/s0...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12331</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/01_tcp_ar/000/00000000/s001...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tuh_eeg/v1.2.0/edf/03_tcp_ar_a/149/00014928/s0...</td>\n",
       "      <td>v1.2.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14928</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path version  year  month  \\\n",
       "0  tuh_eeg/v1.1.0/edf/02_tcp_le/000/00000058/s001...  v1.1.0  2003      2   \n",
       "1  tuh_eeg/v1.1.0/edf/01_tcp_ar/099/00009932/s004...  v1.1.0  2014      9   \n",
       "2  tuh_eeg/v1.1.0/edf/03_tcp_ar_a/123/00012331/s0...  v1.1.0  2014     12   \n",
       "3  tuh_eeg/v1.1.0/edf/01_tcp_ar/000/00000000/s001...  v1.1.0  2015     12   \n",
       "4  tuh_eeg/v1.2.0/edf/03_tcp_ar_a/149/00014928/s0...  v1.2.0  2016      1   \n",
       "\n",
       "   day  subject  session  segment  age gender  \n",
       "0    5       58        1        0    0      M  \n",
       "1   30     9932        4       13   53      F  \n",
       "2   14    12331        3        2   39      M  \n",
       "3   30        0        1        0   37      M  \n",
       "4   15    14928        4        7   83      F  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TUH_PATH = 'edf/train/'\n",
    "tuh = TUH(\n",
    "    path=TUH_PATH,\n",
    "    recording_ids=None,\n",
    "    target_name=('gender'),  # use both age and gender as decoding target\n",
    "    preload=False,\n",
    "    add_physician_reports=False,\n",
    ")\n",
    "tuh.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bda930-7d77-4888-814b-825d1a52d81d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[-2.29440914]\n",
      " [-1.57676045]\n",
      " [-0.32175798]\n",
      " [-1.60447638]\n",
      " [ 1.34726146]\n",
      " [ 1.35778785]\n",
      " [ 0.7181814 ]\n",
      " [-1.25348204]\n",
      " [-0.39194036]\n",
      " [ 0.80359228]\n",
      " [ 0.65341874]\n",
      " [-1.09040041]\n",
      " [ 2.17115861]\n",
      " [-0.90001031]\n",
      " [ 0.59748181]\n",
      " [ 0.58172382]\n",
      " [ 1.81903619]\n",
      " [ 2.14273693]\n",
      " [ 0.94823322]\n",
      " [ 1.32471369]\n",
      " [-1.03531365]]\n",
      "y: F\n"
     ]
    }
   ],
   "source": [
    "x, y = tuh[-1]\n",
    "print('x:', x)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2817e893-401a-4edc-bb61-2e65cf83bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_windows = create_fixed_length_windows(\n",
    "    tuh,\n",
    "    start_offset_samples=0,\n",
    "    stop_offset_samples=None,\n",
    "    window_size_samples=1000,\n",
    "    window_stride_samples=1000,\n",
    "    drop_last_window=False,\n",
    "    mapping={'M': 0, 'F': 1},  # map non-digit targets\n",
    ")\n",
    "# store the number of windows required for loading later on\n",
    "tuh_windows.set_description({\n",
    "    \"n_windows\": [len(d) for d in tuh_windows.datasets]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777b2f3c-4c62-4c2b-b586-a624c434fd69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[ 0.95744467  0.36770442  0.20845759 ... -1.0786871  -0.61109\n",
      "  -2.294409  ]\n",
      " [-1.1028883   1.0068827  -1.1743277  ...  0.771221   -1.3385423\n",
      "  -1.5767604 ]\n",
      " [ 0.27898267  0.3863388  -0.2679523  ...  2.9042215  -0.33213955\n",
      "  -0.32175797]\n",
      " ...\n",
      " [ 1.338765   -0.22695391  0.34812006 ...  0.10860196  0.2009789\n",
      "   0.94823325]\n",
      " [ 1.5344774  -0.991343   -0.02419238 ... -0.64405024 -1.1567156\n",
      "   1.3247137 ]\n",
      " [-0.4681851   1.0513295  -0.43032187 ...  0.20396124  0.15305676\n",
      "  -1.0353136 ]]\n",
      "y: 1\n",
      "ind: [3, 2600, 3600]\n"
     ]
    }
   ],
   "source": [
    "x, y, ind = tuh_windows[-1]\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('ind:', ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3a60a78-14bb-4c6b-b8f2-3cfddce91903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X: tensor([[[-1.2357,  0.6497, -0.8300,  ..., -0.7639,  0.0224,  0.3111],\n",
      "         [-0.8634,  0.5799, -0.3252,  ..., -0.9947, -1.6502, -1.1434],\n",
      "         [ 0.0595, -0.1587, -1.6838,  ...,  0.5377, -0.4645,  0.4578],\n",
      "         ...,\n",
      "         [ 0.2055, -0.9867, -0.8866,  ..., -0.1409,  2.0982,  0.8535],\n",
      "         [ 0.4756,  0.2467, -0.3098,  ...,  0.0427,  0.8393, -0.7588],\n",
      "         [-0.3236,  0.6192,  1.4197,  ...,  0.5149, -0.2055,  0.5038]],\n",
      "\n",
      "        [[-0.7843, -0.4270, -0.0223,  ..., -0.3390,  0.5544, -0.2855],\n",
      "         [ 0.4028, -0.1024,  1.1524,  ...,  1.5556,  0.4898,  1.3321],\n",
      "         [-1.1180, -1.0933, -0.2780,  ...,  0.0785,  0.9793,  1.2615],\n",
      "         ...,\n",
      "         [ 0.0842,  0.1249,  0.5489,  ..., -1.2234, -0.4355,  0.3331],\n",
      "         [ 0.2752,  0.3448,  0.1283,  ...,  2.1729,  1.3250,  1.0280],\n",
      "         [ 0.9441,  1.0467,  0.1602,  ..., -0.7809, -0.9615,  0.8170]],\n",
      "\n",
      "        [[-0.3747, -1.1219,  1.6295,  ...,  0.8105,  0.4693,  0.3093],\n",
      "         [-0.5658, -0.8672,  0.3879,  ...,  0.6243,  1.6806,  1.4981],\n",
      "         [ 0.9808, -0.7801, -0.0674,  ...,  0.7112, -1.8904, -1.2388],\n",
      "         ...,\n",
      "         [ 0.2336,  0.6200, -0.3358,  ...,  1.3705, -0.4479, -0.0333],\n",
      "         [ 0.6031, -1.1994, -1.0637,  ...,  0.1991,  1.3806, -0.2379],\n",
      "         [-1.5661,  0.8940,  1.2789,  ...,  0.5795,  0.1084,  0.1150]],\n",
      "\n",
      "        [[ 0.9574,  0.3677,  0.2085,  ..., -1.0787, -0.6111, -2.2944],\n",
      "         [-1.1029,  1.0069, -1.1743,  ...,  0.7712, -1.3385, -1.5768],\n",
      "         [ 0.2790,  0.3863, -0.2680,  ...,  2.9042, -0.3321, -0.3218],\n",
      "         ...,\n",
      "         [ 1.3388, -0.2270,  0.3481,  ...,  0.1086,  0.2010,  0.9482],\n",
      "         [ 1.5345, -0.9913, -0.0242,  ..., -0.6441, -1.1567,  1.3247],\n",
      "         [-0.4682,  1.0513, -0.4303,  ...,  0.2040,  0.1531, -1.0353]]])\n",
      "batch_y: tensor([1, 1, 1, 1])\n",
      "batch_ind: [tensor([0, 1, 2, 3]), tensor([   0, 1000, 2000, 2600]), tensor([1000, 2000, 3000, 3600])]\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(\n",
    "    dataset=tuh_windows,\n",
    "    batch_size=4,\n",
    ")\n",
    "for batch_X, batch_y, batch_ind in dl:\n",
    "    pass\n",
    "print('batch_X:', batch_X)\n",
    "print('batch_y:', batch_y)\n",
    "print('batch_ind:', batch_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ae30e0-54c7-48f0-ae4a-0ce3b89d6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>version</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>segment</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/02_tcp_le/000/00000058/s001...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/01_tcp_ar/099/00009932/s004...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>9932</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/03_tcp_ar_a/123/00012331/s0...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12331</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tuh_eeg/v1.1.0/edf/01_tcp_ar/000/00000000/s001...</td>\n",
       "      <td>v1.1.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tuh_eeg/v1.2.0/edf/03_tcp_ar_a/149/00014928/s0...</td>\n",
       "      <td>v1.2.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14928</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path version  year  month  \\\n",
       "0  tuh_eeg/v1.1.0/edf/02_tcp_le/000/00000058/s001...  v1.1.0  2003      2   \n",
       "1  tuh_eeg/v1.1.0/edf/01_tcp_ar/099/00009932/s004...  v1.1.0  2014      9   \n",
       "2  tuh_eeg/v1.1.0/edf/03_tcp_ar_a/123/00012331/s0...  v1.1.0  2014     12   \n",
       "3  tuh_eeg/v1.1.0/edf/01_tcp_ar/000/00000000/s001...  v1.1.0  2015     12   \n",
       "4  tuh_eeg/v1.2.0/edf/03_tcp_ar_a/149/00014928/s0...  v1.2.0  2016      1   \n",
       "\n",
       "   day  subject  session  segment  age gender  \n",
       "0    5       58        1        0    0      M  \n",
       "1   30     9932        4       13   53      F  \n",
       "2   14    12331        3        2   39      M  \n",
       "3   30        0        1        0   37      M  \n",
       "4   15    14928        4        7   83      F  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TUH_PATH = 'edf/dev/'\n",
    "tuh1 = TUH(\n",
    "    path=TUH_PATH,\n",
    "    recording_ids=None,\n",
    "    target_name=('gender'),  # use both age and gender as decoding target\n",
    "    preload=False,\n",
    "    add_physician_reports=False,\n",
    ")\n",
    "tuh1.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17bf4bdf-41d1-4e9d-9eb9-3ec2a18c421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuh_windows1 = create_fixed_length_windows(\n",
    "    tuh1,\n",
    "    start_offset_samples=0,\n",
    "    stop_offset_samples=None,\n",
    "    window_size_samples=1000,\n",
    "    window_stride_samples=1000,\n",
    "    drop_last_window=False,\n",
    "    mapping={'M': 0, 'F': 1},  # map non-digit targets\n",
    ")\n",
    "# store the number of windows required for loading later on\n",
    "tuh_windows1.set_description({\n",
    "    \"n_windows\": [len(d) for d in tuh_windows1.datasets]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228ced36-f2da-4ce6-b7e8-62410c0552c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseConcatDataset at 0x7f295f6544d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuh_windows1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6810945c-f429-4fdb-a3a2-37b6b79f000e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X: torch.Size([4, 21, 1000])\n",
      "batch_y: tensor([1, 1, 1, 1])\n",
      "batch_ind: [tensor([0, 1, 2, 3]), tensor([   0, 1000, 2000, 2600]), tensor([1000, 2000, 3000, 3600])]\n"
     ]
    }
   ],
   "source": [
    "dll = DataLoader(\n",
    "    dataset=tuh_windows1,\n",
    "    batch_size=4,\n",
    ")\n",
    "for batch_X1, batch_y1, batch_ind1 in dll:\n",
    "    batch_X1\n",
    "print('batch_X:', batch_X.shape)\n",
    "print('batch_y:', batch_y)\n",
    "print('batch_ind:', batch_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92b1fd3e-97eb-4170-bd3d-74233a00c1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (20, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _ = x.shape\n",
    "        # print('x',x.shape)\n",
    "        x = x[ :,None, :, :]\n",
    "        # print('x',x.shape)\n",
    "        x = self.shallownet(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # global average pooling\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4880, 256),#4x4880\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return x, out\n",
    "\n",
    "\n",
    "class Conformer(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, depth=6, n_classes=2, **kwargs):\n",
    "        super().__init__(\n",
    "\n",
    "            PatchEmbedding(emb_size),\n",
    "            TransformerEncoder(depth, emb_size),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76247778-0822-4d70-927f-5e313ced3cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExP():\n",
    "    def __init__(self, nsub):\n",
    "        super(ExP, self).__init__()\n",
    "        self.batch_size = 72\n",
    "        self.n_epochs = 500\n",
    "        self.c_dim = 4\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.dimension = (190, 50)\n",
    "        self.nSub = nsub\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.root = '/Data/strict_TE/'\n",
    "\n",
    "        # self.log_write = open(\"./results/log_subject%d.txt\" % self.nSub, \"w\")\n",
    "\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "        self.criterion_l1 = torch.nn.L1Loss().cuda()\n",
    "        self.criterion_l2 = torch.nn.MSELoss().cuda()\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "        gpus = [0]\n",
    "        self.model = Conformer().cuda()\n",
    "        self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
    "        self.model = self.model.cuda()\n",
    "        # summary(self.model, (1, 22, 1000))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "\n",
    "        self.dataloader = DataLoader(dataset=tuh_windows,batch_size=4,)#torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.test_dataloader = DataLoader(dataset=tuh_windows1,batch_size=4,)#torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        bestAcc = 0\n",
    "        averAcc = 0\n",
    "        num = 0\n",
    "        Y_true = 0\n",
    "        Y_pred = 0\n",
    "\n",
    "        # Train the cnn model\n",
    "        total_step = len(self.dataloader)\n",
    "        curr_lr = self.lr\n",
    "\n",
    "        for e in range(self.n_epochs):\n",
    "            pred = []\n",
    "            true = []\n",
    "            \n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            for img, label, i in self.dataloader:\n",
    "\n",
    "                img = Variable(img.cuda().type(self.Tensor))\n",
    "                label = Variable(label.cuda().type(self.LongTensor))\n",
    "\n",
    "                # data augmentation\n",
    "                # aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # img = torch.cat((img, aug_data))\n",
    "                # label = torch.cat((label, aug_label))\n",
    "\n",
    "\n",
    "                tok, outputs = self.model(img)\n",
    "\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print('epoch',e)\n",
    "            print('loss', loss)\n",
    "            # out_epoch = time.time()\n",
    "\n",
    "#             for test_data, test_label, i in self.test_dataloader:\n",
    "#             # test process\n",
    "#                 if (e + 1) % 1 == 0:\n",
    "#                     self.model.eval()\n",
    "#                     Tok, Cls = self.model(test_data)\n",
    "\n",
    "\n",
    "#                     loss_test = self.criterion_cls(Cls, test_label)\n",
    "#                     y_pred = torch.max(Cls, 1)[1]\n",
    "#                     pred.append(y_pred)\n",
    "#                     true.append(test_label)\n",
    "                    \n",
    "#             acc = float((pred == true).cpu().numpy().astype(int).sum()) / float(len(true))\n",
    "#             # train_pred = torch.max(outputs, 1)[1]\n",
    "#             # train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "\n",
    "#             print('Epoch:', e,\n",
    "#                   '  Train loss: %.6f' % loss.detach().cpu().numpy(),\n",
    "#                   '  Test loss: %.6f' % loss_test.detach().cpu().numpy(),\n",
    "#                   # '  Train accuracy %.6f' % train_acc,\n",
    "#                   '  Test accuracy is %.6f' % acc)\n",
    "\n",
    "#             # self.log_write.write(str(e) + \"    \" + str(acc) + \"\\n\")\n",
    "#             num = num + 1\n",
    "#             averAcc = averAcc + acc\n",
    "#             if acc > bestAcc:\n",
    "#                 bestAcc = acc\n",
    "#                 Y_true = true\n",
    "#                 Y_pred = pred\n",
    "\n",
    "\n",
    "        torch.save(self.model.module.state_dict(), 'model.pth')\n",
    "#         averAcc = averAcc / num\n",
    "#         print('The average accuracy is:', averAcc)\n",
    "#         print('The best accuracy is:', bestAcc)\n",
    "#         # self.log_write.write('The average accuracy is: ' + str(averAcc) + \"\\n\")\n",
    "#         # self.log_write.write('The best accuracy is: ' + str(bestAcc) + \"\\n\")\n",
    "\n",
    "#         return bestAcc, averAcc, Y_true, Y_pred\n",
    "#         # writer.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13c30fcf-4651-4427-a345-cd8075af8e96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  1 09:54:02 2023\n",
      "seed is 1585\n",
      "Subject 1\n",
      "epoch 0\n",
      "loss tensor(2.2751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 1\n",
      "loss tensor(1.6702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 2\n",
      "loss tensor(1.3254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 3\n",
      "loss tensor(1.8329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 4\n",
      "loss tensor(1.4305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 5\n",
      "loss tensor(1.5214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 6\n",
      "loss tensor(1.8577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 7\n",
      "loss tensor(1.2755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 8\n",
      "loss tensor(1.8299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 9\n",
      "loss tensor(1.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 10\n",
      "loss tensor(0.9433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 11\n",
      "loss tensor(1.6243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 12\n",
      "loss tensor(0.9573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 13\n",
      "loss tensor(1.0854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 14\n",
      "loss tensor(1.1140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 15\n",
      "loss tensor(0.9519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 16\n",
      "loss tensor(0.9176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 17\n",
      "loss tensor(1.1448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 18\n",
      "loss tensor(0.9851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 19\n",
      "loss tensor(1.5320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 20\n",
      "loss tensor(0.9415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 21\n",
      "loss tensor(0.9575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 22\n",
      "loss tensor(1.2024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 23\n",
      "loss tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 24\n",
      "loss tensor(0.9432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 25\n",
      "loss tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 26\n",
      "loss tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 27\n",
      "loss tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 28\n",
      "loss tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 29\n",
      "loss tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 30\n",
      "loss tensor(0.5105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 31\n",
      "loss tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 32\n",
      "loss tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 33\n",
      "loss tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 34\n",
      "loss tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 35\n",
      "loss tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 36\n",
      "loss tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 37\n",
      "loss tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 38\n",
      "loss tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 39\n",
      "loss tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 40\n",
      "loss tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 41\n",
      "loss tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 42\n",
      "loss tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 43\n",
      "loss tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 44\n",
      "loss tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 45\n",
      "loss tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 46\n",
      "loss tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 47\n",
      "loss tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 48\n",
      "loss tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 49\n",
      "loss tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 50\n",
      "loss tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 51\n",
      "loss tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 52\n",
      "loss tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 53\n",
      "loss tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 54\n",
      "loss tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 55\n",
      "loss tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 56\n",
      "loss tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 57\n",
      "loss tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 58\n",
      "loss tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 59\n",
      "loss tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 60\n",
      "loss tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 61\n",
      "loss tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 62\n",
      "loss tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 63\n",
      "loss tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 64\n",
      "loss tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 65\n",
      "loss tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 66\n",
      "loss tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 67\n",
      "loss tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 68\n",
      "loss tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 69\n",
      "loss tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 70\n",
      "loss tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 71\n",
      "loss tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 72\n",
      "loss tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 73\n",
      "loss tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 74\n",
      "loss tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 75\n",
      "loss tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 76\n",
      "loss tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 77\n",
      "loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 78\n",
      "loss tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 79\n",
      "loss tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 80\n",
      "loss tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 81\n",
      "loss tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 82\n",
      "loss tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 83\n",
      "loss tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 84\n",
      "loss tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 85\n",
      "loss tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 86\n",
      "loss tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 87\n",
      "loss tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 88\n",
      "loss tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 89\n",
      "loss tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 90\n",
      "loss tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 91\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 92\n",
      "loss tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 93\n",
      "loss tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 94\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 95\n",
      "loss tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 96\n",
      "loss tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 97\n",
      "loss tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 98\n",
      "loss tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 99\n",
      "loss tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 100\n",
      "loss tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 101\n",
      "loss tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 102\n",
      "loss tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 103\n",
      "loss tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 104\n",
      "loss tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 105\n",
      "loss tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 106\n",
      "loss tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 107\n",
      "loss tensor(5.6831e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 108\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 109\n",
      "loss tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 110\n",
      "loss tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 111\n",
      "loss tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 112\n",
      "loss tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 113\n",
      "loss tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 114\n",
      "loss tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 115\n",
      "loss tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 116\n",
      "loss tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 117\n",
      "loss tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 118\n",
      "loss tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 119\n",
      "loss tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 120\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 121\n",
      "loss tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 122\n",
      "loss tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 123\n",
      "loss tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 124\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 125\n",
      "loss tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 126\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 127\n",
      "loss tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 128\n",
      "loss tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 129\n",
      "loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 130\n",
      "loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 131\n",
      "loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 132\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 133\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 134\n",
      "loss tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 135\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 136\n",
      "loss tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 137\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 138\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 139\n",
      "loss tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 140\n",
      "loss tensor(1.8269e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 141\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 142\n",
      "loss tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 143\n",
      "loss tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 144\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 145\n",
      "loss tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 146\n",
      "loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 147\n",
      "loss tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 148\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 149\n",
      "loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 150\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 151\n",
      "loss tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 152\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 153\n",
      "loss tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 154\n",
      "loss tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 155\n",
      "loss tensor(4.4255e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 156\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 157\n",
      "loss tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 158\n",
      "loss tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 159\n",
      "loss tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 160\n",
      "loss tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 161\n",
      "loss tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 162\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 163\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 164\n",
      "loss tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 165\n",
      "loss tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 166\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 167\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 168\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 169\n",
      "loss tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 170\n",
      "loss tensor(1.1444e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 171\n",
      "loss tensor(5.9809e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 172\n",
      "loss tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 173\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 174\n",
      "loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 175\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 176\n",
      "loss tensor(9.3453e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 177\n",
      "loss tensor(5.8440e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 178\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 179\n",
      "loss tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 180\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 181\n",
      "loss tensor(5.4563e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 182\n",
      "loss tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 183\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 184\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 185\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 186\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 187\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 188\n",
      "loss tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 189\n",
      "loss tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 190\n",
      "loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 191\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 192\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 193\n",
      "loss tensor(6.2373e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 194\n",
      "loss tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 195\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 196\n",
      "loss tensor(4.5893e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 197\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 198\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 199\n",
      "loss tensor(8.9755e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 200\n",
      "loss tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 201\n",
      "loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 202\n",
      "loss tensor(5.8710e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 203\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 204\n",
      "loss tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 205\n",
      "loss tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 206\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 207\n",
      "loss tensor(2.6523e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 208\n",
      "loss tensor(1.8328e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 209\n",
      "loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 210\n",
      "loss tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 211\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 212\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 213\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 214\n",
      "loss tensor(4.5267e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 215\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 216\n",
      "loss tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 217\n",
      "loss tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 218\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 219\n",
      "loss tensor(8.3380e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 220\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 221\n",
      "loss tensor(1.6480e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 222\n",
      "loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 223\n",
      "loss tensor(7.7064e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 224\n",
      "loss tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 225\n",
      "loss tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 226\n",
      "loss tensor(1.0908e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 227\n",
      "loss tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 228\n",
      "loss tensor(5.9902e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 229\n",
      "loss tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 230\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 231\n",
      "loss tensor(4.1573e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 232\n",
      "loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 233\n",
      "loss tensor(6.4873e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 234\n",
      "loss tensor(8.0457e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 235\n",
      "loss tensor(1.1682e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 236\n",
      "loss tensor(7.4261e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 237\n",
      "loss tensor(1.9937e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 238\n",
      "loss tensor(4.6728e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 239\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 240\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 241\n",
      "loss tensor(6.8390e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 242\n",
      "loss tensor(7.7485e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 243\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 244\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 245\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 246\n",
      "loss tensor(1.4067e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 247\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 248\n",
      "loss tensor(5.0244e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 249\n",
      "loss tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 250\n",
      "loss tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 251\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 252\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 253\n",
      "loss tensor(8.9044e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 254\n",
      "loss tensor(3.6088e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 255\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 256\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 257\n",
      "loss tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 258\n",
      "loss tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 259\n",
      "loss tensor(1.4841e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 260\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 261\n",
      "loss tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 262\n",
      "loss tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 263\n",
      "loss tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 264\n",
      "loss tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 265\n",
      "loss tensor(1.5497e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 266\n",
      "loss tensor(4.4253e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 267\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 268\n",
      "loss tensor(1.5139e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 269\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 270\n",
      "loss tensor(2.5421e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 271\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 272\n",
      "loss tensor(6.9584e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 273\n",
      "loss tensor(2.5868e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 274\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 275\n",
      "loss tensor(6.7196e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 276\n",
      "loss tensor(1.2606e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 277\n",
      "loss tensor(8.9407e-08, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 278\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 279\n",
      "loss tensor(3.5403e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 280\n",
      "loss tensor(8.3320e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 281\n",
      "loss tensor(7.2799e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 282\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 283\n",
      "loss tensor(1.4633e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 284\n",
      "loss tensor(1.2815e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 285\n",
      "loss tensor(2.0473e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 286\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 287\n",
      "loss tensor(4.2108e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 288\n",
      "loss tensor(5.5638e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 289\n",
      "loss tensor(2.4706e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 290\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 291\n",
      "loss tensor(8.6592e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 292\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 293\n",
      "loss tensor(9.2982e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 294\n",
      "loss tensor(5.0068e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 295\n",
      "loss tensor(2.6226e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 296\n",
      "loss tensor(2.3782e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 297\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 298\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 299\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 300\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 301\n",
      "loss tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 302\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 303\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 304\n",
      "loss tensor(6.1954e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 305\n",
      "loss tensor(5.3701e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 306\n",
      "loss tensor(2.2321e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 307\n",
      "loss tensor(5.6624e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 308\n",
      "loss tensor(7.7713e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 309\n",
      "loss tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 310\n",
      "loss tensor(2.0266e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 311\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 312\n",
      "loss tensor(2.2948e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 313\n",
      "loss tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 314\n",
      "loss tensor(1.3143e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 315\n",
      "loss tensor(4.6491e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 316\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 317\n",
      "loss tensor(8.0466e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 318\n",
      "loss tensor(3.4480e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 319\n",
      "loss tensor(1.8268e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 320\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 321\n",
      "loss tensor(8.7613e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 322\n",
      "loss tensor(2.1189e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 323\n",
      "loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 324\n",
      "loss tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 325\n",
      "loss tensor(1.2517e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 326\n",
      "loss tensor(6.0911e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 327\n",
      "loss tensor(1.5437e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 328\n",
      "loss tensor(1.2606e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 329\n",
      "loss tensor(4.2555e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 330\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 331\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 332\n",
      "loss tensor(6.0284e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 333\n",
      "loss tensor(7.9273e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 334\n",
      "loss tensor(1.2129e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 335\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 336\n",
      "loss tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 337\n",
      "loss tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 338\n",
      "loss tensor(8.2552e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 339\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 340\n",
      "loss tensor(1.2248e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 341\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 342\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 343\n",
      "loss tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 344\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 345\n",
      "loss tensor(9.8348e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 346\n",
      "loss tensor(8.6145e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 347\n",
      "loss tensor(1.4454e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 348\n",
      "loss tensor(1.1593e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 349\n",
      "loss tensor(6.7440e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 350\n",
      "loss tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 351\n",
      "loss tensor(2.3842e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 352\n",
      "loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 353\n",
      "loss tensor(8.4042e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 354\n",
      "loss tensor(2.9384e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 355\n",
      "loss tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 356\n",
      "loss tensor(3.3854e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 357\n",
      "loss tensor(1.0490e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 358\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 359\n",
      "loss tensor(2.2530e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 360\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 361\n",
      "loss tensor(7.7534e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 362\n",
      "loss tensor(5.7990e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 363\n",
      "loss tensor(1.6331e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 364\n",
      "loss tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 365\n",
      "loss tensor(2.2619e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 366\n",
      "loss tensor(4.4461e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 367\n",
      "loss tensor(5.3818e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 368\n",
      "loss tensor(6.1924e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 369\n",
      "loss tensor(5.2452e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 370\n",
      "loss tensor(1.3798e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 371\n",
      "loss tensor(4.2585e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 372\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 373\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 374\n",
      "loss tensor(1.0520e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 375\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 376\n",
      "loss tensor(4.4611e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 377\n",
      "loss tensor(3.6358e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 378\n",
      "loss tensor(2.3782e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 379\n",
      "loss tensor(2.7716e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 380\n",
      "loss tensor(1.1861e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 381\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 382\n",
      "loss tensor(8.5161e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 383\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 384\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 385\n",
      "loss tensor(1.2219e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 386\n",
      "loss tensor(2.9146e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 387\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 388\n",
      "loss tensor(9.1492e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 389\n",
      "loss tensor(8.3290e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 390\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 391\n",
      "loss tensor(9.3280e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 392\n",
      "loss tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 393\n",
      "loss tensor(3.9041e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 394\n",
      "loss tensor(4.9472e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 395\n",
      "loss tensor(8.0466e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 396\n",
      "loss tensor(1.4305e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 397\n",
      "loss tensor(8.7022e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 398\n",
      "loss tensor(1.1921e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 399\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 400\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 401\n",
      "loss tensor(2.2948e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 402\n",
      "loss tensor(3.2783e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 403\n",
      "loss tensor(4.4969e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 404\n",
      "loss tensor(9.4472e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 405\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 406\n",
      "loss tensor(4.8275e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 407\n",
      "loss tensor(3.2484e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 408\n",
      "loss tensor(2.3215e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 409\n",
      "loss tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 410\n",
      "loss tensor(1.1921e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 411\n",
      "loss tensor(1.7344e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 412\n",
      "loss tensor(5.1856e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 413\n",
      "loss tensor(1.0043e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 414\n",
      "loss tensor(1.0192e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 415\n",
      "loss tensor(6.6459e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 416\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 417\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 418\n",
      "loss tensor(1.9073e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 419\n",
      "loss tensor(1.8507e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 420\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 421\n",
      "loss tensor(5.9840e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 422\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 423\n",
      "loss tensor(3.2186e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 424\n",
      "loss tensor(1.3113e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 425\n",
      "loss tensor(6.3478e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 426\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 427\n",
      "loss tensor(1.1921e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 428\n",
      "loss tensor(1.7881e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 429\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 430\n",
      "loss tensor(5.6798e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 431\n",
      "loss tensor(9.7882e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 432\n",
      "loss tensor(6.4933e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 433\n",
      "loss tensor(1.1801e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 434\n",
      "loss tensor(2.6822e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 435\n",
      "loss tensor(5.5730e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 436\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 437\n",
      "loss tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 438\n",
      "loss tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 439\n",
      "loss tensor(9.5367e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 440\n",
      "loss tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 441\n",
      "loss tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 442\n",
      "loss tensor(5.9780e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 443\n",
      "loss tensor(5.6624e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 444\n",
      "loss tensor(1.2219e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 445\n",
      "loss tensor(3.9753e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 446\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 447\n",
      "loss tensor(5.9605e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 448\n",
      "loss tensor(1.7881e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 449\n",
      "loss tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 450\n",
      "loss tensor(5.5307e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 451\n",
      "loss tensor(1.1921e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 452\n",
      "loss tensor(3.2155e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 453\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 454\n",
      "loss tensor(2.2590e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 455\n",
      "loss tensor(8.3148e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 456\n",
      "loss tensor(5.6624e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 457\n",
      "loss tensor(7.9571e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 458\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 459\n",
      "loss tensor(4.4703e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 460\n",
      "loss tensor(3.5763e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 461\n",
      "loss tensor(5.9605e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 462\n",
      "loss tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 463\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 464\n",
      "loss tensor(2.7418e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 465\n",
      "loss tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 466\n",
      "loss tensor(1.7881e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 467\n",
      "loss tensor(4.8007e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 468\n",
      "loss tensor(3.3585e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 469\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 470\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 471\n",
      "loss tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 472\n",
      "loss tensor(3.6955e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 473\n",
      "loss tensor(1.3768e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 474\n",
      "loss tensor(3.3080e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 475\n",
      "loss tensor(1.6689e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 476\n",
      "loss tensor(6.0498e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 477\n",
      "loss tensor(3.3704e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 478\n",
      "loss tensor(2.1160e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 479\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 480\n",
      "loss tensor(1.6689e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 481\n",
      "loss tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 482\n",
      "loss tensor(1.6093e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 483\n",
      "loss tensor(7.7486e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 484\n",
      "loss tensor(3.6061e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 485\n",
      "loss tensor(1.0311e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 486\n",
      "loss tensor(2.3871e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 487\n",
      "loss tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 488\n",
      "loss tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 489\n",
      "loss tensor(6.4038e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 490\n",
      "loss tensor(3.2484e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 491\n",
      "loss tensor(5.1046e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 492\n",
      "loss tensor(2.6910e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 493\n",
      "loss tensor(2.3842e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 494\n",
      "loss tensor(7.5508e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 495\n",
      "loss tensor(5.5730e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 496\n",
      "loss tensor(5.3644e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 497\n",
      "loss tensor(1.2517e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 498\n",
      "loss tensor(1.1921e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 499\n",
      "loss tensor(2.6524e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Wed Mar  1 09:54:58 2023\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    best = 0\n",
    "    aver = 0\n",
    "    # result_write = open(\"preprocess/sub_result.txt\", \"w\")\n",
    "\n",
    "    for i in range(1):\n",
    "        starttime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "        seed_n = np.random.randint(2021)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "\n",
    "\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1)\n",
    "\n",
    "        # bestAcc, averAcc, Y_true, Y_pred = \n",
    "        exp.train()\n",
    "#         print('THE BEST ACCURACY IS ' + str(bestAcc))\n",
    "#         # result_write.write('Subject ' + str(i + 1) + ' : ' + 'Seed is: ' + str(seed_n) + \"\\n\")\n",
    "#         # result_write.write('Subject ' + str(i + 1) + ' : ' + 'The best accuracy is: ' + str(bestAcc) + \"\\n\")\n",
    "#         # result_write.write('Subject ' + str(i + 1) + ' : ' + 'The average accuracy is: ' + str(averAcc) + \"\\n\")\n",
    "\n",
    "#         endtime = datetime.datetime.now()\n",
    "#         print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "#         best = best + bestAcc\n",
    "#         aver = aver + averAcc\n",
    "#         if i == 0:\n",
    "#             yt = Y_true\n",
    "#             yp = Y_pred\n",
    "#         else:\n",
    "#             yt = torch.cat((yt, Y_true))\n",
    "#             yp = torch.cat((yp, Y_pred))\n",
    "\n",
    "\n",
    "#     best = best / 9\n",
    "#     aver = aver / 9\n",
    "#     print('average best accuracy', best)\n",
    "#     print('average accuracy', aver)\n",
    "    # result_write.write('**The average Best accuracy is: ' + str(best) + \"\\n\")\n",
    "    # result_write.write('The average Aver accuracy is: ' + str(aver) + \"\\n\")\n",
    "    # result_write.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(time.asctime(time.localtime(time.time())))\n",
    "    main()\n",
    "    print(time.asctime(time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "764f1bd9-9577-4110-9b3d-e455d13fcc8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conformer(\n",
       "  (0): PatchEmbedding(\n",
       "    (shallownet): Sequential(\n",
       "      (0): Conv2d(1, 40, kernel_size=(1, 25), stride=(1, 1))\n",
       "      (1): Conv2d(40, 40, kernel_size=(20, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): AvgPool2d(kernel_size=(1, 75), stride=(1, 15), padding=0)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (projection): Sequential(\n",
       "      (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Rearrange('b e (h) (w) -> b (h w) e')\n",
       "    )\n",
       "  )\n",
       "  (1): TransformerEncoder(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (queries): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (values): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=40, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=160, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (queries): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (values): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=40, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=160, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (queries): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (values): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=40, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=160, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (queries): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (values): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=40, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=160, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (queries): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (values): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=40, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=160, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerEncoderBlock(\n",
       "      (0): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MultiHeadAttention(\n",
       "            (keys): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (queries): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (values): Linear(in_features=40, out_features=40, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=40, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): FeedForwardBlock(\n",
       "            (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "            (1): GELU(approximate=none)\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=160, out_features=40, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): ClassificationHead(\n",
       "    (clshead): Sequential(\n",
       "      (0): Reduce('b n e -> b e', 'mean')\n",
       "      (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=40, out_features=2, bias=True)\n",
       "    )\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=4880, out_features=256, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=32, bias=True)\n",
       "      (4): ELU(alpha=1.0)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=32, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Conformer()\n",
    "#self.model = nn.DataParallel(self.model, device_ids=[i for i in range(len(gpus))])\n",
    "model = model#.cuda()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e52d46f-a25d-424d-b041-4886a5c7ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "true =[]\n",
    "for test_data, test_label, batch_ind1 in dll:\n",
    "    Tok, Cls = model(test_data)\n",
    "    # loss_test = self.criterion_cls(Cls, test_label)\n",
    "    y_pred = torch.max(Cls, 1)[1]\n",
    "    pred.append(y_pred.numpy())\n",
    "    true.append(test_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "182d719e-7fe5-4328-bfb6-fc79a8253ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = float((np.concatenate(pred) == np.concatenate(true)).sum()) / float(len(true))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "877d0aad-94bc-42c8-a300-31d2bad6118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.concatenate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194604b4-55fa-4ee0-93bd-6f59d0f0c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
